{
  "models": [
    {
      "id": "Tongyi-MAI/Z-Image-Turbo",
      "author": "Tongyi-MAI",
      "author_fullname": "Tongyi-MAI",
      "name": "Z-Image-Turbo",
      "url": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Tongyi-MAI/Z-Image-Turbo.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "downloads": 86570,
      "likes": 1835,
      "pipeline_tag": "text-to-image",
      "num_parameters": null,
      "last_modified": "2025-12-02T11:54:58.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n\n<h1 align=\"center\">\u26a1\ufe0f- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![GitHub](https://img.shields.io/badge/GitHub-Z--Image-181717?logo=github&logoColor=white)](https://github.com/Tongyi-MAI/Z-Image)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Mobile_Demo-Z--Image--Turbo-red)](https://huggingface.co/spaces/akhaliq/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/\ud83e\udd16%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Image\uff08\u9020\u76f8\uff09project!\n\n</div>\n\n\n\n## \u2728 Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- \ud83d\ude80 **Z-Image-Turbo** \u2013 A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **\u26a1\ufe0fsub-second inference latency\u26a1\ufe0f** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- \ud83e\uddf1 **Z-Image-Base** \u2013 The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- \u270d\ufe0f **Z-Image-Edit** \u2013 A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### \ud83d\udce5 Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) <br> [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Online%20Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo) | [![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) <br> [![ModelScope Space](https://img.shields.io/badge/%F0%9F%A4%96%20Online%20Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%3A%2F%2FTongyi-MAI%2FZ-Image-Turbo%3Frevision%3Dmaster) |\n| **Z-Image-Base** | *To be released*                                                                                                                                                                                                                                                                                                          | *To be released*                                                                                                                                                                                                                                                                      ",
      "description": "Z-Image-Turbo is an efficient text-to-image diffusion transformer model optimized for speed and resource usage, achieving sub-second inference with 8 NFEs and fitting within 16GB VRAM. It excels at photorealistic generation, bilingual text rendering (English/Chinese), and strong instruction adherence, making it suitable for rapid content creation on consumer hardware."
    },
    {
      "id": "black-forest-labs/FLUX.2-dev",
      "author": "black-forest-labs",
      "author_fullname": "Black Forest Labs",
      "name": "FLUX.2-dev",
      "url": "https://huggingface.co/black-forest-labs/FLUX.2-dev",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/black-forest-labs/FLUX.2-dev.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f7a8f4be90e06da248e0f/m5YoF33abJ09vcwFxt1Mj.png",
      "downloads": 180864,
      "likes": 833,
      "pipeline_tag": "image-to-image",
      "num_parameters": null,
      "last_modified": "2025-11-27T11:17:09.000Z",
      "readme_content": "---\nlanguage:\n- en\nlicense: other\nlicense_name: flux-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt\nextra_gated_prompt: >-\n  By clicking \"Agree\", you agree to the [FLUX [dev] Non-Commercial License\n  Agreement](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt)\n  and acknowledge the [Acceptable Use\n  Policy](https://bfl.ai/legal/usage-policy).\ntags:\n- image-generation\n- image-editing\n- flux\n- diffusion-single-file\npipeline_tag: image-to-image\nlibrary_name: diffusers\n---\n\n![Teaser](./teaser_generation.png)\n![Teaser](./teaser_editing.png)\n\n`FLUX.2 [dev]` is a 32 billion parameter rectified flow transformer capable of generating, editing and combining images based on text instructions.\nFor more information, please read our [blog post](https://bfl.ai/blog/flux-2).\n\n# Key Features\n1. State of the art in open text-to-image generation, single-reference editing and multi-reference editing.\n2. No need for finetuning: character, object and style reference without additional training in one model.\n4. Trained using guidance distillation, making `FLUX.2 [dev]` more efficient.\n5. Open weights to drive new scientific research, and empower artists to develop innovative workflows.\n6. Generated outputs can be used for personal, scientific, and commercial purposes, as described in the [FLUX \\[dev\\] Non-Commercial License](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev).\n\n# Usage\nWe provide a reference implementation of `FLUX.2 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux2).\nDevelopers and creatives looking to build on top of `FLUX.2 [dev]` are encouraged to use this as a starting point.\n\n`FLUX.2 [dev]` is also available in both [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [Diffusers](https://github.com/huggingface/diffusers).\n\n### Using with diffusers \ud83e\udde8\n\nFor local deployment on a consumer type graphics card, like an RTX 4090 or an RTX 5090, please see the [diffusers docs](https://github.com/black-forest-labs/flux2/blob/main/docs/flux2_dev_hf.md) on our GitHub page.\n\nAs an example, here's a way to load a 4-bit quantized model with a remote text-encoder on an RTX 4090:\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\" #quantized text-encoder and DiT. VAE still in bf16\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that start with #FF5733 at the top and transitions to #33FF57 at the bottom.\"\n\n#cat_image = load_image(\"https://huggingface.co/spaces/zerogpu-aoti/FLUX.1-Kontext-Dev-fp8-dynamic/resolve/main/cat.png\")\nimage = pipe(\n    prompt_embeds=remote_text_encoder(prompt),\n    #image=[cat_image] #optional multi-image input\n    generator=torch.Generator(device=device).manual_seed(42),\n    num_inference_steps=50, #28 steps can be a good trade-off\n    guidance_scale=4,\n).images[0]\n\nimage.save(\"flux2_output.png\")\n```\n\n\n---\n\n# Risks\n\nBlack Forest Labs is committed to the responsible development and deployment of our models. Prior to releasing the FLUX.2 family of models, we evaluated and mitigated a number of risks in our model checkpoints and hosted services, including the generation of unlawful content such as child sexual abuse material (CSAM) and nonconsensual intimate imagery (NCII). We implemented a series of pre-release mitigations to help prevent misuse by third parties, with additional post-release mitigations to help address residual risks:\n1. Pre-training mitigation. We filtered pre-training data for multiple categories of \u201cnot safe for work\u201d (NSFW) and known child sexual abuse material (CSAM) to help prevent a user generating unlawful content in response to text prompts or uploaded images. We have partnered with the Internet Watch Foundation, an independent nonprofit organization dedicated to preventing online abuse, to filter known CSAM from the training data.\n2. Post-training mitigation. Subsequently, we undertook multiple rounds of targeted fine-tuning to provide additional mitigation against potential abuse, including both text-to-image (T2I) and image-to-image (I2I) attacks. By inhibiting certain behaviors and suppressing certain concepts in the trained model, these techniques can help to prevent a user generating synthetic CSAM or NCII from a text prompt, or transforming an uploaded image into synthetic CSAM or NCII.\n3. Ongoing evaluation. Throughout this process, we conducted multiple internal and external third-party evaluations of model checkpoints to identify further opportunities for mitigation. External third-party evaluations focused on eliciting CSAM and NCII through adversarial testing with (i) text-only prompts, (ii) a single uploaded reference image with text prompts, and (iii) multiple uploaded reference images with text prompts. Based on this feedback, we conducted further safety fine-tuning to produce our open-weight model (FLUX.2 [dev]).\n4. Release decision. After safety fine-tuning and",
      "description": "FLUX.2-dev is a 32B parameter rectified flow transformer for advanced image generation and editing, excelling at text-to-image, single/multi-reference editing without finetuning, and style/character transfer."
    },
    {
      "id": "deepseek-ai/DeepSeek-Math-V2",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-Math-V2",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-Math-V2",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-Math-V2.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 5732,
      "likes": 593,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-11-27T10:35:52.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-Math-V2\n---\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/Competitions.png\">\n</p>\n\n## 4. Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base.\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThis repository and the model weights are licensed under [the Apache License, Version 2.0 (Apache 2.0)](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).\n",
      "description": "DeepSeek-Math-V2 is a large language model specialized in mathematical reasoning and theorem proving, achieving state-of-the-art results on competitions like IMO and Putnam by employing a self-verification mechanism to ensure proof rigor."
    },
    {
      "id": "deepseek-ai/DeepSeek-V3.2",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-V3.2",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-V3.2.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 2894,
      "likes": 558,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-12-01T11:04:59.000Z",
      "readme_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* \ud83e\udd47 **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>hello<\uff5cAssistant\uff5c></think>Hello! I am DeepSeek.<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output ",
      "description": "DeepSeek-V3.2 is an efficient text generation model excelling in reasoning and agentic tasks, featuring DeepSeek Sparse Attention for long contexts and advanced RL training that rivals GPT-5, making it suitable for complex problem-solving and tool-use scenarios."
    },
    {
      "id": "tencent/HunyuanOCR",
      "author": "tencent",
      "author_fullname": "Tencent",
      "name": "HunyuanOCR",
      "url": "https://huggingface.co/tencent/HunyuanOCR",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/tencent/HunyuanOCR.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png",
      "downloads": 185857,
      "likes": 601,
      "pipeline_tag": "image-text-to-text",
      "num_parameters": "996.2M",
      "last_modified": "2025-12-02T02:23:08.000Z",
      "readme_content": "---\nlicense: other\nlanguage:\n- multilingual\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n- tencent/HunyuanOCR\ntags:\n- ocr\n- hunyuan\n- vision-language\n- image-to-text\n- 1B\n- end-to-end\n---\n\n<p align=\"center\">\n <img src=\"https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true\" width=\"80%\"/> <br>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/tencent/HunyuanOCR\"><b>\ud83c\udfaf Demo</b></a> |\n<a href=\"https://huggingface.co/tencent/HunyuanOCR\"><b>\ud83d\udce5 Model Download</b></a> |\n<a href=\"https://arxiv.org/abs/2511.19575\"><b>\ud83d\udcc4 Technical Report</b></a> |\n<a href=\"https://github.com/Tencent-Hunyuan/HunyuanOCR\"><b>\ud83c\udf1f Github</b></a>\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2511.19575\">HunyuanOCR</a>\n</p>\n</h2>\n\n\n## \ud83d\udcd6 Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan's native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## \ud83d\ude80 Quick Start with Transformers\n\n### Installation\n```bash\npip install git+https://github.com/huggingface/transformers@82a06db03535c49aa987719ed0746a76093b1ec4\n```\n> **Note**: We will merge it into the Transformers main branch later.\n\n### Model Inference\n\n```python\nfrom transformers import AutoProcessor\nfrom transformers import HunYuanVLForConditionalGeneration\nfrom PIL import Image\nimport torch\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_name_or_path = \"tencent/HunyuanOCR\"\nprocessor = AutoProcessor.from_pretrained(model_name_or_path, use_fast=False)\nimg_path = \"path/to/your/image.jpg\"\nimage_inputs = Image.open(img_path)\nmessages1 = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": img_path},\n            {\"type\": \"text\", \"text\": (\n                \"\u68c0\u6d4b\u5e76\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\uff0c\u5c06\u6587\u672c\u5750\u6807\u683c\u5f0f\u5316\u8f93\u51fa\u3002\"\n            )},\n        ],\n    }\n]\nmessages = [messages1]\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\nmodel = HunYuanVLForConditionalGeneration.from_pretrained(\n    model_name_or_path,\n    attn_implementation=\"eager\",\n    dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\nwith torch.no_grad():\n    device = next(model.parameters()).device\n    inputs = inputs.to(device)\n    generated_ids = model.generate(**inputs, max_new_tokens=16384, do_sample=False)\nif \"input_ids\" in inputs:\n    input_ids = inputs.input_ids\nelse:\n    print(\"inputs: # fallback\", inputs)\n    input_ids = inputs.inputs\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)\n]\noutput_texts = clean_repeated_substrings(processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n))\nprint(output_texts)\n```\n\n\n## \ud83d\ude80 Quick Start with vLLM\n\nCheckout [vLLM HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html).\n\n### Installation\n\n```bash\nuv venv hunyuanocr\nsource hunyuanocr/bin/activate\n\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\\n    --no-enable-prefix-caching \\\n    --mm-processor-cache-gb 0 \\\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = \"tencent/HunyuanOCR\"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = \"/path/to/image.jpg\"\nimg = Image.open(img_path)\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": img_path},\n        {\"type\": \"text\", \"text\": \"\u68c0\u6d4b\u5e76\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\uff0c\u5c06\u6587\u672c\u5750\u6807\u683c\u5f0f\u5316\u8f93\u51fa\u3002\"}\n    ]}\n]\nprompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [img]}}\noutput = llm.generate([inputs], sampling_params)[0]\nprint(clean_repeated_substrings(output.outputs[0].text))\n```\n\n## \ud83d\udcac Application-oriented Prompts\n\n| Task | English | Chinese |\n|------|---------|---------|\n| **Spotting** | Detect and recognize text in the image, and output the text coordinates in a formatted manner. ",
      "description": "HunyuanOCR is a lightweight 1B parameter VLM for complex multilingual document parsing, excelling in text spotting, information extraction, video subtitle extraction, and photo translation."
    },
    {
      "id": "deepseek-ai/DeepSeek-V3.2-Speciale",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-V3.2-Speciale",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-V3.2-Speciale.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 1052,
      "likes": 395,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-12-01T11:06:03.000Z",
      "readme_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* \ud83e\udd47 **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>hello<\uff5cAssistant\uff5c></think>Hello! I am DeepSeek.<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output ",
      "description": "DeepSeek-V3.2-Speciale is a highly efficient text generation model fine-tuned from DeepSeek-V3.2-Exp-Base, excelling in reasoning and agentic tasks with performance surpassing GPT-5. It features DeepSeek Sparse Attention for long contexts and a scalable RL framework, making it suitable for complex interactive environments and competitive programming benchmarks."
    },
    {
      "id": "Comfy-Org/z_image_turbo",
      "author": "Comfy-Org",
      "author_fullname": "Comfy Org",
      "name": "z_image_turbo",
      "url": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Comfy-Org/z_image_turbo.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/63462c815efccdc07f175568/3ZxGxVKftaTTjduryN7g7.png",
      "downloads": 1227087,
      "likes": 325,
      "pipeline_tag": null,
      "num_parameters": null,
      "last_modified": "2025-11-27T04:15:24.000Z",
      "readme_content": "---\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nWorkflows: https://comfyanonymous.github.io/ComfyUI_examples/z_image/",
      "description": "z_image_turbo is a diffusion model designed for single-file image generation, likely integrated with ComfyUI workflows for advanced image synthesis tasks."
    },
    {
      "id": "microsoft/Fara-7B",
      "author": "microsoft",
      "author_fullname": "Microsoft",
      "name": "Fara-7B",
      "url": "https://huggingface.co/microsoft/Fara-7B",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/microsoft/Fara-7B.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "downloads": 22418,
      "likes": 376,
      "pipeline_tag": "image-text-to-text",
      "num_parameters": "8.3B",
      "last_modified": "2025-12-01T15:20:36.000Z",
      "readme_content": "---\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\ntags:\n- multimodal\n---\n\n# Fara-7B: An Efficient Agentic Model for Computer Use\n\n[![Microsoft](https://img.shields.io/badge/Microsoft-Project-0078D4?logo=microsoft)](https://aka.ms/msaif/fara)\n[![Hugging Face Dataset](https://img.shields.io/badge/\ud83e\udd17-Dataset-yellow)](https://huggingface.co/datasets/microsoft/WebTailBench)\n[![Foundry](https://img.shields.io/badge/Azure-Foundry-0089D6)](https://aka.ms/foundry-fara-7b)\n[![Github](https://img.shields.io/badge/Github-181717?logo=github&logoColor=white)](https://github.com/microsoft/fara)\n[![Paper](https://img.shields.io/badge/Paper-2511.19663-red)](https://huggingface.co/papers/2511.19663)\n\n[Official Microsoft Blog](https://www.microsoft.com/en-us/research/?p=1155843&preview=1&_ppp=0a22f3e916)<br>\n[Technical Report](https://aka.ms/fara-techreport)<br>\n[Paper](https://huggingface.co/papers/2511.19663)<br>\n[Github](https://github.com/microsoft/fara)<br>\n[Microsoft Foundry](https://ai.azure.com/explore/models/Fara-7B/version/1/registry/azureml-msr?tid=72f988bf-86f1-41af-91ab-2d7cd011db47)<br>\n\n## Model Summary\n\n**Developer:** Microsoft Research  \n\n**Description:**  \nFara-7B is Microsoft's first agentic small language model (SLM) designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.\n\n**Model Architecture:**  \nMultimodal decoder-only language model that takes an image (screenshot) + text context. It directly predicts thoughts and actions with grounded arguments. Current production baselines leverage Qwen 2.5-VL (7B).\n\n**Parameters:** 7 Billion  \n\n**Inputs:** User goal (text), current screenshot(s), history of previous outputs (thoughts + actions text) from the agent.  \n\n**Context Length:** 128k  \n\n**Outputs:** Generated text in response to the input, with a chain-of-thought block followed by a tool call block to indicate the action.  \n\n**GPUs:** 64 H100s  \n\n**Training Time:** 2.5 days  \n\n**Public Data Summary:** N/A  \n\n**Dates:** Trained between 26th October 2025 to 29th October 2025  \n\n**Status:** Static model trained on public and private data  \n\n**Release Date:** November 24th, 2025  \n\n**License:** MIT  \n\n**Model Dependencies:** Qwen 2.5 VL  \n\n**Additional Assets:** N/A  \n\n**Acceptable Use Policy:** N/A  \n\n---\n\n## 1. Model Overview\n\nFara is a 7B Computer Use Agent (CUA) model specialized for taking actions on the web to accomplish high-level user tasks. Beyond understanding webpage layout and basic action mechanics, it plans and executes high-level goals like booking restaurants, applying for jobs, planning trips, and buying shopping lists. Its training relies on a large-scale, fully synthetic dataset of action trajectories generated and verified by a multi-agent pipeline.  \n\nFara perceives browser inputs via screenshots, while internal reasoning and state history are recorded textually. Based on recent screenshots and a full history of actions, it predicts the next action with necessary arguments (e.g., coordinates for clicks).\n\n### 1.1 Alignment Approach\n\nFara-7B uses a robust post-training safety approach leveraging open-source and in-house synthetic datasets. It incorporates critical point recognition\u2014situations requiring user permission or sensitive information\u2014to safely halt actions. The model is trained to refuse harmful tasks and undergoes automated red teaming to assess risks, including grounding, jailbreaks, harmful content, and copyright violations.\n\n### 1.2 Safeguards\n\nFara-7B is trained to refuse tasks in categories that violate usage policy:\n\n| Type | Description | Examples |\n|------|------------|---------|\n| Illegal Activities | Tasks requiring unlawful actions | Terrorism-related searches, piracy, unauthorized access, weapons creation |\n| Deceptive Tasks | Tasks misleading or impersonating | Fake forms, fraudulent listings, phishing |\n| High-Risk/Regulated Domains | Tasks requiring professional oversight | Medical, legal, financial advice or approvals |\n| Harassment, Exploitation, Hate | Tasks harming or discriminating | Harassment content, stalking, sexualizing minors |\n| Unsafe Technical Use | Misuse of automation | Large-scale scraping, spam, system disruption |\n| Misinformation | Spreading false claims | Publishing unverified claims |\n| Sexual | Erotic or pornographic tasks | Erotic roleplay, porn searches |\n\nCritical points where the agent stops include entering personal info, completing purchases, making calls, sending emails, submitting applications, and signing into accounts.\n\n---\n\n## 2. Usage\n\n### Sample Usage\n\nYou can try Fara-7B locally by setting up the environment and hosting the model. For full instructions, refer to the [GitHub repository](https://github.com/microsoft/fara#installation).\n\n```bash\n# 1. Clone repository\ngit clone https://github.com/microsoft/fara.git\ncd fara\n\n# 2. Setup environment\npython3 -m venv .venv \nsource .venv/bin/activate\npip install -e .\nplaywright install\n```\n\nThen in one process, host the model:\n```bash\nvllm serve \"microsoft/Fara-7B\" --port 5000 --dtype auto \n```\nThen you can iterative query it with:\n```bash\nfara-cli --task \"whats the weather in new york now\"\n```\n\nHint: might need to do `--tensor-parallel-size 2` with vllm command if you run out of memory\n\n### 2.1 Primary Use Cases\n\n- Automating web tasks such as shopping, booking travel, restaurant reservations, info-seeking, or account workflows.  \n- Performs actions step-by-step using multimodal understanding from browser screenshots.  \n- On-device execution provides privacy guarantees and lower latency.\n\n### 2.2 Out-of-Scope Use Cases\n\n- Model not evaluated for all downstream purposes; consider limitations of LLMs for accuracy, safety, and fairness.  \n- Must adhere to applicable laws and regulations.  \n- English-only support.  \n\n### 2.3 Distr",
      "description": "Fara-7B is a 7B parameter multimodal agentic SLM designed for computer use, capable of understanding screenshots and text to perform complex web tasks like booking and shopping by predicting thoughts and actions with grounded arguments."
    },
    {
      "id": "nvidia/Orchestrator-8B",
      "author": "nvidia",
      "author_fullname": "NVIDIA",
      "name": "Orchestrator-8B",
      "url": "https://huggingface.co/nvidia/Orchestrator-8B",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/nvidia/Orchestrator-8B.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1613114437487-60262a8e0703121c822a80b6.png",
      "downloads": 670,
      "likes": 247,
      "pipeline_tag": "text-generation",
      "num_parameters": "8.2B",
      "last_modified": "2025-12-02T06:56:58.000Z",
      "readme_content": "---\nlibrary_name: transformers\nbase_model:\n- Qwen/Qwen3-8B\n---\n# ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration\n\n[![Paper](https://img.shields.io/badge/ArXiv-Paper-brown)](https://arxiv.org/abs/2511.21689)\n[![Code](https://img.shields.io/badge/GitHub-Link-orange)](https://github.com/NVlabs/ToolOrchestra/)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-green)](https://huggingface.co/nvidia/Orchestrator-8B)\n[![Data](https://img.shields.io/badge/HuggingFace-Data-blue)](https://huggingface.co/datasets/nvidia/ToolScale)\n[![Website](https://img.shields.io/badge/Web-Page-purple)](https://research.nvidia.com/labs/lpr/ToolOrchestra/)\n\n\n### Description\n\nOrchestrator-8B is a state-of-the-art 8B parameter orchestration model designed to solve complex, multi-turn agentic tasks by coordinating a diverse set of expert models and tools.\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/method.png\" width=\"100%\"/>\n<p>\n\n\nOn the Humanity's Last Exam (HLE) benchmark, ToolOrchestrator-8B achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being approximately 2.5x more efficient.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/HLE_benchmark.png\" width=\"80%\"/>\n<p>\n\nThis model is for research and development only.\n\n\n### Key Features\n\n- Intelligent Orchestration: Capable of managing heterogeneous toolsets including basic tools (search, code execution) and other LLMs (specialized and generalist).\n- Multi-Objective RL Training: Trained via Group Relative Policy Optimization (GRPO) with a novel reward function that optimizes for accuracy, latency/cost, and adherence to user preferences.\n- Efficiency: Delivers higher accuracy at significantly lower computational cost compared to monolithic frontier models.\n- Robust Generalization: Demonstrated ability to generalize to unseen tools and pricing configurations.\n\n### Benchmark\nOn Humanity\u2019s Last Exam, Orchestrator-8B achieves 37.1%, surpassing GPT-5 (35.1%) with only 30% monetary cost and 2.5x faster. On FRAMES and \u03c4\u00b2-Bench, Orchestrator-8B consistently outperforms strong monolithic systems, demonstrating versatile reasoning and robust tool orchestration.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/results.png\" width=\"100%\"/>\n<p>\n\nOrchestrator-8B consistently outperforms GPT-5, Claude Opus 4.1 and Qwen3-235B-A22B on HLE with substantially lower cost.\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/cost_performance.png\" width=\"60%\"/>\n<p>\n\n\n### Model Details\n\n- Developed by: NVIDIA & University of Hong Kong \n- Model Type: Decoder-only Transformer\n- Base Model: [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) \n- Parameters: 8B\n- Language(s): English\n- License: NVIDIA License\n\n### Model Version(s):\n1.0 <br>\n\n### Training Dataset:\n**Link:** \n| Dataset                      | Link                                                                                         | \n|---------------------------|-------------------------------------------------------------------------------------------|\n| GeneralThought-430K  | [Link](https://huggingface.co/datasets/natolambert/GeneralThought-430K-filtered)                   |\n| ToolScale           | [Link](https://huggingface.co/datasets/nvidia/ToolScale)                            |\n\n\n\n### Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. <br> \n\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns [here](https://app.intigriti.com/programs/nvidia/nvidiavdp/detail).\n\n\n### License/Terms of Use\n[NVIDIA License](LICENSE)\n\n\n### Citation\nIf you find this model useful, please cite our [paper](https://arxiv.org/abs/2511.21689):\n```\n@misc{toolorchestra,\n      title={ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration}, \n      author={Hongjin Su and Shizhe Diao and Ximing Lu and Mingjie Liu and Jiacheng Xu and Xin Dong and Yonggan Fu and Peter Belcak and Hanrong Ye and Hongxu Yin and Yi Dong and Evelina Bakhturina and Tao Yu and Yejin Choi and Jan Kautz and Pavlo Molchanov},\n      year={2025},\n      eprint={2511.21689},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2511.21689}, \n}\n```",
      "description": "Orchestrator-8B is an 8B parameter model that intelligently coordinates expert models and tools to solve complex agentic tasks, achieving higher accuracy and efficiency than frontier models on benchmarks like Humanity's Last Exam."
    },
    {
      "id": "facebook/sam3",
      "author": "facebook",
      "author_fullname": "AI at Meta",
      "name": "sam3",
      "url": "https://huggingface.co/facebook/sam3",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/facebook/sam3.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1592839207516-noauth.png",
      "downloads": 296084,
      "likes": 847,
      "pipeline_tag": "mask-generation",
      "num_parameters": "859.9M",
      "last_modified": "2025-11-20T22:05:08.000Z",
      "readme_content": "---\nlicense: other\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlanguage:\n- en\npipeline_tag: mask-generation\nlibrary_name: transformers\ntags:\n- sam3\n---\n\nSAM 3 is a unified foundation model for promptable segmentation in images and videos. It can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor [SAM 2](https://github.com/facebookresearch/sam2), SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars. Unlike prior work, SAM 3 can handle a vastly larger set of open-vocabulary prompts. It achieves 75-80% of human performance on our new [SA-CO benchmark](https://github.com/facebookresearch/sam3/edit/main_readme/README.md#sa-co-dataset) which contains 270K unique concepts, over 50 times more than existing benchmarks.\n\n[Hugging Face \ud83e\udd17  app](https://huggingface.co/spaces/akhaliq/sam3)\n\n### Basic Usage\n\n```python\nimport torch\n#################################### For Image ####################################\nfrom PIL import Image\nfrom sam3.model_builder import build_sam3_image_model\nfrom sam3.model.sam3_image_processor import Sam3Processor\n# Load the model\nmodel = build_sam3_image_model()\nprocessor = Sam3Processor(model)\n# Load an image\nimage = Image.open(\"<YOUR_IMAGE_PATH.jpg>\")\ninference_state = processor.set_image(image)\n# Prompt the model with text\noutput = processor.set_text_prompt(state=inference_state, prompt=\"<YOUR_TEXT_PROMPT>\")\n\n# Get the masks, bounding boxes, and scores\nmasks, boxes, scores = output[\"masks\"], output[\"boxes\"], output[\"scores\"]\n\n#################################### For Video ####################################\n\nfrom sam3.model_builder import build_sam3_video_predictor\n\nvideo_predictor = build_sam3_video_predictor()\nvideo_path = \"<YOUR_VIDEO_PATH>\" # a JPEG folder or an MP4 video file\n# Start a session\nresponse = video_predictor.handle_request(\n    request=dict(\n        type=\"start_session\",\n        resource_path=video_path,\n    )\n)\nresponse = video_predictor.handle_request(\n    request=dict(\n        type=\"add_prompt\",\n        session_id=response[\"session_id\"],\n        frame_index=0, # Arbitrary frame index\n        text=\"<YOUR_TEXT_PROMPT>\",\n    )\n)\noutput = response[\"outputs\"]\n```\n\nThe official code is publicly released in the [sam3 repo](https://github.com/facebookresearch/sam3).\n\n\n## Usage with \ud83e\udd17 Transformers\n\n### SAM3 - Promptable Concept Segmentation (PCS) for Images\n\nSAM3 performs Promptable Concept Segmentation (PCS) on images, taking text and/or image exemplars as prompts and returning segmentation masks for **all matching object instances** in the image.\n\n#### Text-Only Prompts\n\n```python\n>>> from transformers import Sam3Processor, Sam3Model\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n\n>>> # Load image\n>>> image_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n>>> image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n\n>>> # Segment using text prompt\n>>> inputs = processor(images=image, text=\"ear\", return_tensors=\"pt\").to(device)\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Post-process results\n>>> results = processor.post_process_instance_segmentation(\n...     outputs,\n...     threshold=0.5,\n...     mask_threshold=0.5,\n...     target_sizes=inputs.get(\"original_sizes\").tolist()\n... )[0]\n\n>>> print(f\"Found {len(results['masks'])} objects\")\n>>> # Results contain:\n>>> # - masks: Binary masks resized to original image size\n>>> # - boxes: Bounding boxes in absolute pixel coordinates (xyxy format)\n>>> # - scores: Confidence scores\n```\n\nYou can display masks using a simple helper like the following:\n\n```python\nimport numpy as np\nimport matplotlib\n\ndef overlay_masks(image, masks):\n    image = image.convert(\"RGBA\")\n    masks = 255 * masks.cpu().numpy().astype(np.uint8)\n    \n    n_masks = masks.shape[0]\n    cmap = matplotlib.colormaps.get_cmap(\"rainbow\").resampled(n_masks)\n    colors = [\n        tuple(int(c * 255) for c in cmap(i)[:3])\n        for i in range(n_masks)\n    ]\n\n    for mask, color in zip(masks, colors):\n        mask = Image.fromarray(mask)\n        overlay = Image.new(\"RGBA\", image.size, color + (0,))\n        alpha = mask.point(lambda v: int(v * 0.5))\n        overlay.putalpha(alpha)\n        image = Image.alpha_composite(image, overlay)\n    return image\n```\n\nThen you can save the resulting composite image or display it in a notebook:\n\n```python\n>>> overlay_masks(image, results[\"masks\"])\n```\n\n#### Single Bounding Box Prompt\n\nSegment objects using a bounding box:\n\n```python\n>>> # Box in xyxy format: [x1, y1, x2, y2] in pixel coordinates\n>>> # Example: laptop region\n>>> box_xyxy = [100, 150, 500, 450]\n>>> input_boxes = [[box_xyxy]]  # [batch, num_boxes, 4]\n>>> input_boxes_labels = [[1]]  # 1 = positive box\n\n>>> inputs = processor(\n...     images=image,\n...     input_boxes=input_boxes,\n...     input_boxes_labels=input_boxes_labels,\n...     return_tensors=\"pt\"\n... ).to(device)\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Post-process re",
      "description": "SAM 3 is a unified foundation model for promptable segmentation in images and videos, capable of detecting, segmenting, and tracking objects using text or visual prompts. It excels at exhaustively segmenting all instances of open-vocabulary concepts, achieving near-human performance on complex benchmarks."
    }
  ],
  "limit": 10,
  "fetched_at": "2025-12-02T17:32:20.831005+00:00",
  "has_descriptions": true
}