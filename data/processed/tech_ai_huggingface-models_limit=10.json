{
  "models": [
    {
      "id": "Tongyi-MAI/Z-Image-Turbo",
      "author": "Tongyi-MAI",
      "author_fullname": "Tongyi-MAI",
      "name": "Z-Image-Turbo",
      "url": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Tongyi-MAI/Z-Image-Turbo.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "downloads": 31451,
      "likes": 1365,
      "pipeline_tag": "text-to-image",
      "num_parameters": null,
      "last_modified": "2025-11-28T13:15:09.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n\n<h1 align=\"center\">\u26a1\ufe0f- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![GitHub](https://img.shields.io/badge/GitHub-Z--Image-181717?logo=github&logoColor=white)](https://github.com/Tongyi-MAI/Z-Image)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Mobile_Demo-Z--Image--Turbo-red)](https://huggingface.co/spaces/akhaliq/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/\ud83e\udd16%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"http://github.com/Tongyi-MAI/Z-Image/blob/main/Z_Image_Report.pdf\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Image\uff08\u9020\u76f8\uff09project!\n\n</div>\n\n\n\n## \u2728 Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- \ud83d\ude80 **Z-Image-Turbo** \u2013 A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **\u26a1\ufe0fsub-second inference latency\u26a1\ufe0f** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- \ud83e\uddf1 **Z-Image-Base** \u2013 The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- \u270d\ufe0f **Z-Image-Edit** \u2013 A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### \ud83d\udce5 Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) <br> [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Online%20Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo) | [![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) <br> [![ModelScope Space](https://img.shields.io/badge/%F0%9F%A4%96%20Online%20Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%3A%2F%2FTongyi-MAI%2FZ-Image-Turbo%3Frevision%3Dmaster) |\n| **Z-Image-Base** | *To be released*                                                                                                                                                                                                                                                                                                          | *To be released*                                                                                                                                                                                                                                     ",
      "description": "Z-Image-Turbo is an efficient text-to-image diffusion transformer model optimized for speed and resource usage, achieving sub-second inference with 8 NFEs and fitting within 16GB VRAM. It excels at photorealistic generation, bilingual text rendering (English/Chinese), and strong instruction adherence, making it suitable for rapid content creation on consumer hardware."
    },
    {
      "id": "black-forest-labs/FLUX.2-dev",
      "author": "black-forest-labs",
      "author_fullname": "Black Forest Labs",
      "name": "FLUX.2-dev",
      "url": "https://huggingface.co/black-forest-labs/FLUX.2-dev",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/black-forest-labs/FLUX.2-dev.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f7a8f4be90e06da248e0f/m5YoF33abJ09vcwFxt1Mj.png",
      "downloads": 168078,
      "likes": 733,
      "pipeline_tag": "image-to-image",
      "num_parameters": null,
      "last_modified": "2025-11-27T11:17:09.000Z",
      "readme_content": "---\nlanguage:\n- en\nlicense: other\nlicense_name: flux-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt\nextra_gated_prompt: >-\n  By clicking \"Agree\", you agree to the [FLUX [dev] Non-Commercial License\n  Agreement](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt)\n  and acknowledge the [Acceptable Use\n  Policy](https://bfl.ai/legal/usage-policy).\ntags:\n- image-generation\n- image-editing\n- flux\n- diffusion-single-file\npipeline_tag: image-to-image\nlibrary_name: diffusers\n---\n\n![Teaser](./teaser_generation.png)\n![Teaser](./teaser_editing.png)\n\n`FLUX.2 [dev]` is a 32 billion parameter rectified flow transformer capable of generating, editing and combining images based on text instructions.\nFor more information, please read our [blog post](https://bfl.ai/blog/flux-2).\n\n# Key Features\n1. State of the art in open text-to-image generation, single-reference editing and multi-reference editing.\n2. No need for finetuning: character, object and style reference without additional training in one model.\n4. Trained using guidance distillation, making `FLUX.2 [dev]` more efficient.\n5. Open weights to drive new scientific research, and empower artists to develop innovative workflows.\n6. Generated outputs can be used for personal, scientific, and commercial purposes, as described in the [FLUX \\[dev\\] Non-Commercial License](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev).\n\n# Usage\nWe provide a reference implementation of `FLUX.2 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux2).\nDevelopers and creatives looking to build on top of `FLUX.2 [dev]` are encouraged to use this as a starting point.\n\n`FLUX.2 [dev]` is also available in both [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [Diffusers](https://github.com/huggingface/diffusers).\n\n### Using with diffusers \ud83e\udde8\n\nFor local deployment on a consumer type graphics card, like an RTX 4090 or an RTX 5090, please see the [diffusers docs](https://github.com/black-forest-labs/flux2/blob/main/docs/flux2_dev_hf.md) on our GitHub page.\n\nAs an example, here's a way to load a 4-bit quantized model with a remote text-encoder on an RTX 4090:\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\" #quantized text-encoder and DiT. VAE still in bf16\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that start with #FF5733 at the top and transitions to #33FF57 at the bottom.\"\n\n#cat_image = load_image(\"https://huggingface.co/spaces/zerogpu-aoti/FLUX.1-Kontext-Dev-fp8-dynamic/resolve/main/cat.png\")\nimage = pipe(\n    prompt_embeds=remote_text_encoder(prompt),\n    #image=[cat_image] #optional multi-image input\n    generator=torch.Generator(device=device).manual_seed(42),\n    num_inference_steps=50, #28 steps can be a good trade-off\n    guidance_scale=4,\n).images[0]\n\nimage.save(\"flux2_output.png\")\n```\n\n\n---\n\n# Risks\n\nBlack Forest Labs is committed to the responsible development and deployment of our models. Prior to releasing the FLUX.2 family of models, we evaluated and mitigated a number of risks in our model checkpoints and hosted services, including the generation of unlawful content such as child sexual abuse material (CSAM) and nonconsensual intimate imagery (NCII). We implemented a series of pre-release mitigations to help prevent misuse by third parties, with additional post-release mitigations to help address residual risks:\n1. Pre-training mitigation. We filtered pre-training data for multiple categories of \u201cnot safe for work\u201d (NSFW) and known child sexual abuse material (CSAM) to help prevent a user generating unlawful content in response to text prompts or uploaded images. We have partnered with the Internet Watch Foundation, an independent nonprofit organization dedicated to preventing online abuse, to filter known CSAM from the training data.\n2. Post-training mitigation. Subsequently, we undertook multiple rounds of targeted fine-tuning to provide additional mitigation against potential abuse, including both text-to-image (T2I) and image-to-image (I2I) attacks. By inhibiting certain behaviors and suppressing certain concepts in the trained model, these techniques can help to prevent a user generating synthetic CSAM or NCII from a text prompt, or transforming an uploaded image into synthetic CSAM or NCII.\n3. Ongoing evaluation. Throughout this process, we conducted multiple internal and external third-party evaluations of model checkpoints to identify further opportunities for mitigation. External third-party evaluations focused on eliciting CSAM and NCII through adversarial testing with (i) text-only prompts, (ii) a single uploaded reference image with text prompts, and (iii) multiple uploaded reference images with text prompts. Based on this feedback, we conducted further safety fine-tuning to produce our open-weight model (FLUX.2 [dev]).\n4. Release decision. After safety fine-tuning and",
      "description": "FLUX.2-dev is a 32B parameter rectified flow transformer for advanced image generation and editing, excelling at text-to-image, single/multi-reference editing without finetuning, and style/character transfer."
    },
    {
      "id": "tencent/HunyuanOCR",
      "author": "tencent",
      "author_fullname": "Tencent",
      "name": "HunyuanOCR",
      "url": "https://huggingface.co/tencent/HunyuanOCR",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/tencent/HunyuanOCR.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png",
      "downloads": 59610,
      "likes": 521,
      "pipeline_tag": "image-text-to-text",
      "num_parameters": "996.2M",
      "last_modified": "2025-11-27T14:01:50.000Z",
      "readme_content": "---\nlicense: other\nlanguage:\n- zh\n- en\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n\n<p align=\"center\">\n <img src=\"https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true\" width=\"80%\"/> <br>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://huggingface.co/spaces/tencent/HunyuanOCR\"><b>\ud83c\udfaf Demo</b></a> |\n<a href=\"https://huggingface.co/tencent/HunyuanOCR\"><b>\ud83d\udce5 Model Download</b></a> |\n<a href=\"https://arxiv.org/abs/2511.19575\"><b>\ud83d\udcc4 Technical Report</b></a> |\n<a href=\"https://github.com/Tencent-Hunyuan/HunyuanOCR\"><b>\ud83c\udf1f Github</b></a>\n</p>\n\n<h2>\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2511.19575\">HunyuanOCR</a>\n</p>\n</h2>\n\n\n## \ud83d\udcd6 Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan's native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## \ud83d\ude80 Quick Start with Transformers\n\n### Installation\n```bash\npip install git+https://github.com/huggingface/transformers@82a06db03535c49aa987719ed0746a76093b1ec4\n```\n> **Note**: We will merge it into the Transformers main branch later.\n\n### Model Inference\n\n```python\nfrom transformers import AutoProcessor\nfrom transformers import HunYuanVLForConditionalGeneration\nfrom PIL import Image\nimport torch\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_name_or_path = \"tencent/HunyuanOCR\"\nprocessor = AutoProcessor.from_pretrained(model_name_or_path, use_fast=False)\nimg_path = \"path/to/your/image.jpg\"\nimage_inputs = Image.open(img_path)\nmessages1 = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": img_path},\n            {\"type\": \"text\", \"text\": (\n                \"\u68c0\u6d4b\u5e76\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\uff0c\u5c06\u6587\u672c\u5750\u6807\u683c\u5f0f\u5316\u8f93\u51fa\u3002\"\n            )},\n        ],\n    }\n]\nmessages = [messages1]\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\nmodel = HunYuanVLForConditionalGeneration.from_pretrained(\n    model_name_or_path,\n    attn_implementation=\"eager\",\n    dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\nwith torch.no_grad():\n    device = next(model.parameters()).device\n    inputs = inputs.to(device)\n    generated_ids = model.generate(**inputs, max_new_tokens=16384, do_sample=False)\nif \"input_ids\" in inputs:\n    input_ids = inputs.input_ids\nelse:\n    print(\"inputs: # fallback\", inputs)\n    input_ids = inputs.inputs\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)\n]\noutput_texts = clean_repeated_substrings(processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n))\nprint(output_texts)\n```\n\n\n## \ud83d\ude80 Quick Start with vLLM\n\nCheckout [vLLM HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html).\n\n### Installation\n\n```bash\nuv venv hunyuanocr\nsource hunyuanocr/bin/activate\n\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\\n    --no-enable-prefix-caching \\\n    --mm-processor-cache-gb 0 \\\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    \"\"\"Clean repeated substrings in text\"\"\"\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = \"tencent/HunyuanOCR\"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = \"/path/to/image.jpg\"\nimg = Image.open(img_path)\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": img_path},\n        {\"type\": \"text\", \"text\": \"\u68c0\u6d4b\u5e76\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\uff0c\u5c06\u6587\u672c\u5750\u6807\u683c\u5f0f\u5316\u8f93\u51fa\u3002\"}\n    ]}\n]\nprompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [img]}}\noutput = llm.generate([inputs], sampling_params)[0]\nprint(clean_repeated_substrings(output.outputs[0].text))\n```\n\n## \ud83d\udcac Application-oriented Prompts\n\n| Task | English | Chinese |\n|------|---------|---------|\n| **Spotting** | Detect and recognize text in the image, and output the text coordinates in a formatted manner. | \u68c0\u6d4b\u5e76\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\uff0c\u5c06\u6587\u672c\u5750\u6807\u683c\u5f0f\u5316\u8f93\u51fa\u3002 |\n| **Parsing** | \u2022 Identify the formula in the image and represent it using LaTeX for",
      "description": "HunyuanOCR is a lightweight 1B parameter VLM for complex multilingual document parsing, excelling in text spotting, information extraction, video subtitle extraction, and photo translation."
    },
    {
      "id": "deepseek-ai/DeepSeek-Math-V2",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-Math-V2",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-Math-V2",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-Math-V2.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 2642,
      "likes": 477,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-11-27T10:35:52.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-Math-V2\n---\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/Competitions.png\">\n</p>\n\n## 4. Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base.\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThis repository and the model weights are licensed under [the Apache License, Version 2.0 (Apache 2.0)](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).\n",
      "description": "DeepSeek-Math-V2 is a large language model specialized in mathematical reasoning and theorem proving, achieving state-of-the-art results on competitions like IMO and Putnam by employing a self-verification mechanism to ensure proof rigor."
    },
    {
      "id": "microsoft/Fara-7B",
      "author": "microsoft",
      "author_fullname": "Microsoft",
      "name": "Fara-7B",
      "url": "https://huggingface.co/microsoft/Fara-7B",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/microsoft/Fara-7B.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "downloads": 5926,
      "likes": 297,
      "pipeline_tag": "image-text-to-text",
      "num_parameters": "8.3B",
      "last_modified": "2025-11-28T22:44:30.000Z",
      "readme_content": "---\nlicense: mit\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Fara-7B: An Efficient Agentic Model for Computer Use\n\n[![Microsoft](https://img.shields.io/badge/Microsoft-Project-0078D4?logo=microsoft)](https://aka.ms/msaif/fara)\n[![Hugging Face Dataset](https://img.shields.io/badge/\ud83e\udd17-Dataset-yellow)](https://huggingface.co/datasets/microsoft/WebTailBench)\n[![Foundry](https://img.shields.io/badge/Azure-Foundry-0089D6)](https://aka.ms/foundry-fara-7b)\n[![Github](https://img.shields.io/badge/Github-181717?logo=github&logoColor=white)](https://github.com/microsoft/fara)\n\n[Official Microsoft Blog](https://www.microsoft.com/en-us/research/?p=1155843&preview=1&_ppp=0a22f3e916)<br>\n[Technical Report](https://aka.ms/fara-techreport)<br>\n[Github](https://github.com/microsoft/fara)<br>\n[Microsoft Foundry](https://ai.azure.com/explore/models/Fara-7B/version/1/registry/azureml-msr?tid=72f988bf-86f1-41af-91ab-2d7cd011db47)<br>\n\n## Model Summary\n\n**Developer:** Microsoft Research  \n\n**Description:**  \nFara-7B is Microsoft's first agentic small language model (SLM) designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.\n\n**Model Architecture:**  \nMultimodal decoder-only language model that takes an image (screenshot) + text context. It directly predicts thoughts and actions with grounded arguments. Current production baselines leverage Qwen 2.5-VL (7B).\n\n**Parameters:** 7 Billion  \n\n**Inputs:** User goal (text), current screenshot(s), history of previous outputs (thoughts + actions text) from the agent.  \n\n**Context Length:** 128k  \n\n**Outputs:** Generated text in response to the input, with a chain-of-thought block followed by a tool call block to indicate the action.  \n\n**GPUs:** 64 H100s  \n\n**Training Time:** 2.5 days  \n\n**Public Data Summary:** N/A  \n\n**Dates:** Trained between 26th October 2025 to 29th October 2025  \n\n**Status:** Static model trained on public and private data  \n\n**Release Date:** November 24th, 2025  \n\n**License:** MIT  \n\n**Model Dependencies:** Qwen 2.5 VL  \n\n**Additional Assets:** N/A  \n\n**Acceptable Use Policy:** N/A  \n\n---\n\n## 1. Model Overview\n\nFara is a 7B Computer Use Agent (CUA) model specialized for taking actions on the web to accomplish high-level user tasks. Beyond understanding webpage layout and basic action mechanics, it plans and executes high-level goals like booking restaurants, applying for jobs, planning trips, and buying shopping lists. Its training relies on a large-scale, fully synthetic dataset of action trajectories generated and verified by a multi-agent pipeline.  \n\nFara perceives browser inputs via screenshots, while internal reasoning and state history are recorded textually. Based on recent screenshots and a full history of actions, it predicts the next action with necessary arguments (e.g., coordinates for clicks).\n\n### 1.1 Alignment Approach\n\nFara-7B uses a robust post-training safety approach leveraging open-source and in-house synthetic datasets. It incorporates critical point recognition\u2014situations requiring user permission or sensitive information\u2014to safely halt actions. The model is trained to refuse harmful tasks and undergoes automated red teaming to assess risks, including grounding, jailbreaks, harmful content, and copyright violations.\n\n### 1.2 Safeguards\n\nFara-7B is trained to refuse tasks in categories that violate usage policy:\n\n| Type | Description | Examples |\n|------|------------|---------|\n| Illegal Activities | Tasks requiring unlawful actions | Terrorism-related searches, piracy, unauthorized access, weapons creation |\n| Deceptive Tasks | Tasks misleading or impersonating | Fake forms, fraudulent listings, phishing |\n| High-Risk/Regulated Domains | Tasks requiring professional oversight | Medical, legal, financial advice or approvals |\n| Harassment, Exploitation, Hate | Tasks harming or discriminating | Harassment content, stalking, sexualizing minors |\n| Unsafe Technical Use | Misuse of automation | Large-scale scraping, spam, system disruption |\n| Misinformation | Spreading false claims | Publishing unverified claims |\n| Sexual | Erotic or pornographic tasks | Erotic roleplay, porn searches |\n\nCritical points where the agent stops include entering personal info, completing purchases, making calls, sending emails, submitting applications, and signing into accounts.\n\n---\n\n## 2. Usage\n\n### 2.1 Primary Use Cases\n\n- Automating web tasks such as shopping, booking travel, restaurant reservations, info-seeking, or account workflows.  \n- Performs actions step-by-step using multimodal understanding from browser screenshots.  \n- On-device execution provides privacy guarantees and lower latency.\n\n### 2.2 Out-of-Scope Use Cases\n\n- Model not evaluated for all downstream purposes; consider limitations of LLMs for accuracy, safety, and fairness.  \n- Must adhere to applicable laws and regulations.  \n- English-only support.  \n\n### 2.3 Distribution Channels\n\n- Hugging Face  \n- Azure AI Foundry  \n\n### 2.4 Input Formats\n\nGiven the nature of the training data, always use the ChatML template with the following system prompt for inference:\n\n---\n\n**System Prompt:**\n\nYou are a web automation agent that performs actions on websites to fulfill user requests by calling various tools.\n\nYou should stop execution at **Critical Points**. A Critical Point occurs in tasks like:\n\n- Checkout  \n- Book  \n- Purchase  \n- Call  \n- Email  \n- Order  \n\nA Critical Point requires the user's permission or personal/sensitive information (name, email, credit card, address, payment information, resume, etc.) to complete a transaction (purchase, reservation, sign-up, etc.), or to communicate as a human would (call, email, apply to a job, etc.).\n\n**Guideline:** Solve the task as far as possible **up until a Critical Point**",
      "description": "Fara-7B is a 7B parameter multimodal agentic SLM designed for computer use, capable of understanding screenshots and text to perform complex web tasks like booking and shopping by predicting thoughts and actions with grounded arguments."
    },
    {
      "id": "facebook/sam3",
      "author": "facebook",
      "author_fullname": "AI at Meta",
      "name": "sam3",
      "url": "https://huggingface.co/facebook/sam3",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/facebook/sam3.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1592839207516-noauth.png",
      "downloads": 234428,
      "likes": 781,
      "pipeline_tag": "mask-generation",
      "num_parameters": "859.9M",
      "last_modified": "2025-11-20T22:05:08.000Z",
      "readme_content": "---\nlicense: other\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlanguage:\n- en\npipeline_tag: mask-generation\nlibrary_name: transformers\ntags:\n- sam3\n---\n\nSAM 3 is a unified foundation model for promptable segmentation in images and videos. It can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor [SAM 2](https://github.com/facebookresearch/sam2), SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars. Unlike prior work, SAM 3 can handle a vastly larger set of open-vocabulary prompts. It achieves 75-80% of human performance on our new [SA-CO benchmark](https://github.com/facebookresearch/sam3/edit/main_readme/README.md#sa-co-dataset) which contains 270K unique concepts, over 50 times more than existing benchmarks.\n\n[Hugging Face \ud83e\udd17  app](https://huggingface.co/spaces/akhaliq/sam3)\n\n### Basic Usage\n\n```python\nimport torch\n#################################### For Image ####################################\nfrom PIL import Image\nfrom sam3.model_builder import build_sam3_image_model\nfrom sam3.model.sam3_image_processor import Sam3Processor\n# Load the model\nmodel = build_sam3_image_model()\nprocessor = Sam3Processor(model)\n# Load an image\nimage = Image.open(\"<YOUR_IMAGE_PATH.jpg>\")\ninference_state = processor.set_image(image)\n# Prompt the model with text\noutput = processor.set_text_prompt(state=inference_state, prompt=\"<YOUR_TEXT_PROMPT>\")\n\n# Get the masks, bounding boxes, and scores\nmasks, boxes, scores = output[\"masks\"], output[\"boxes\"], output[\"scores\"]\n\n#################################### For Video ####################################\n\nfrom sam3.model_builder import build_sam3_video_predictor\n\nvideo_predictor = build_sam3_video_predictor()\nvideo_path = \"<YOUR_VIDEO_PATH>\" # a JPEG folder or an MP4 video file\n# Start a session\nresponse = video_predictor.handle_request(\n    request=dict(\n        type=\"start_session\",\n        resource_path=video_path,\n    )\n)\nresponse = video_predictor.handle_request(\n    request=dict(\n        type=\"add_prompt\",\n        session_id=response[\"session_id\"],\n        frame_index=0, # Arbitrary frame index\n        text=\"<YOUR_TEXT_PROMPT>\",\n    )\n)\noutput = response[\"outputs\"]\n```\n\nThe official code is publicly released in the [sam3 repo](https://github.com/facebookresearch/sam3).\n\n\n## Usage with \ud83e\udd17 Transformers\n\n### SAM3 - Promptable Concept Segmentation (PCS) for Images\n\nSAM3 performs Promptable Concept Segmentation (PCS) on images, taking text and/or image exemplars as prompts and returning segmentation masks for **all matching object instances** in the image.\n\n#### Text-Only Prompts\n\n```python\n>>> from transformers import Sam3Processor, Sam3Model\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n\n>>> # Load image\n>>> image_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n>>> image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n\n>>> # Segment using text prompt\n>>> inputs = processor(images=image, text=\"ear\", return_tensors=\"pt\").to(device)\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Post-process results\n>>> results = processor.post_process_instance_segmentation(\n...     outputs,\n...     threshold=0.5,\n...     mask_threshold=0.5,\n...     target_sizes=inputs.get(\"original_sizes\").tolist()\n... )[0]\n\n>>> print(f\"Found {len(results['masks'])} objects\")\n>>> # Results contain:\n>>> # - masks: Binary masks resized to original image size\n>>> # - boxes: Bounding boxes in absolute pixel coordinates (xyxy format)\n>>> # - scores: Confidence scores\n```\n\nYou can display masks using a simple helper like the following:\n\n```python\nimport numpy as np\nimport matplotlib\n\ndef overlay_masks(image, masks):\n    image = image.convert(\"RGBA\")\n    masks = 255 * masks.cpu().numpy().astype(np.uint8)\n    \n    n_masks = masks.shape[0]\n    cmap = matplotlib.colormaps.get_cmap(\"rainbow\").resampled(n_masks)\n    colors = [\n        tuple(int(c * 255) for c in cmap(i)[:3])\n        for i in range(n_masks)\n    ]\n\n    for mask, color in zip(masks, colors):\n        mask = Image.fromarray(mask)\n        overlay = Image.new(\"RGBA\", image.size, color + (0,))\n        alpha = mask.point(lambda v: int(v * 0.5))\n        overlay.putalpha(alpha)\n        image = Image.alpha_composite(image, overlay)\n    return image\n```\n\nThen you can save the resulting composite image or display it in a notebook:\n\n```python\n>>> overlay_masks(image, results[\"masks\"])\n```\n\n#### Single Bounding Box Prompt\n\nSegment objects using a bounding box:\n\n```python\n>>> # Box in xyxy format: [x1, y1, x2, y2] in pixel coordinates\n>>> # Example: laptop region\n>>> box_xyxy = [100, 150, 500, 450]\n>>> input_boxes = [[box_xyxy]]  # [batch, num_boxes, 4]\n>>> input_boxes_labels = [[1]]  # 1 = positive box\n\n>>> inputs = processor(\n...     images=image,\n...     input_boxes=input_boxes,\n...     input_boxes_labels=input_boxes_labels,\n...     return_tensors=\"pt\"\n... ).to(device)\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Post-process re",
      "description": "SAM 3 is a unified foundation model for promptable segmentation in images and videos, capable of detecting, segmenting, and tracking objects using text or visual prompts. It excels at exhaustively segmenting all instances of open-vocabulary concepts, achieving near-human performance on complex benchmarks."
    },
    {
      "id": "tencent/HunyuanVideo-1.5",
      "author": "tencent",
      "author_fullname": "Tencent",
      "name": "HunyuanVideo-1.5",
      "url": "https://huggingface.co/tencent/HunyuanVideo-1.5",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/tencent/HunyuanVideo-1.5.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png",
      "downloads": 2337,
      "likes": 769,
      "pipeline_tag": "text-to-video",
      "num_parameters": null,
      "last_modified": "2025-11-26T11:23:16.000Z",
      "readme_content": "---\nlibrary_name: HunyuanVideo-1.5\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - text-to-video\n  - image-to-video\npipeline_tag: text-to-video\nextra_gated_eu_disallowed: true\n---\n\n\n[\u4e2d\u6587\u6587\u6863](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.png\" alt=\"HunyuanVideo-1.5 Logo\" width=\"80%\">\n\n# \ud83c\udfac HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align=\"center\">\n<!-- <img src=\"./assets/banner.png\" alt=\"HunyuanVideo-1.5 Banner\" width=\"800\"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align=\"center\">\n  <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\" target=\"_blank\"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target=\"_blank\"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=\"https://arxiv.org/pdf/2511.18870\" target=\"_blank\"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target=\"_blank\"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md\" target=\"_blank\"><img src=https://img.shields.io/badge/\ud83d\udcda-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href=\"./ComfyUI/README.md\" target=\"_blank\"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href=\"https://github.com/ModelTC/LightX2V\" target=\"_blank\"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href=\"https://tusi.cn/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/\u5410\u53f8-purple.svg?logo=book height=22px></a>\n  <a href=\"https://tensor.art/models/933574988890423836\" target=\"_blank\"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align=\"center\">\n    \ud83d\udc4f Join our <a href=\"./assets/wechat.png\" target=\"_blank\">WeChat</a> and <a href=\"https://discord.gg/ehjWMqF5wY\">Discord</a> | \n\ud83d\udcbb <a href=\"https://hunyuan.tencent.com/video/zh?tabIndex=0\">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## \ud83d\udd25\ud83d\udd25\ud83d\udd25 News\n* \ud83d\ude80 Nov 24, 2025: We now support cache inference, achieving approximately 2x speedup! Pull the latest code to try it. \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83c\udd95 \n* \ud83d\udc4b Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## \ud83c\udfa5 Demo\n<div align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d\" width=\"60%\"> </video>\n</div>\n\n## \ud83e\udde9 Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **ComfyUI** - [ComfyUI](https://github.com/comfyanonymous/ComfyUI): A powerful and modular diffusion model GUI with a graph/nodes interface. ComfyUI supports HunyuanVideo-1.5 with various engineering optimizations for fast inference. We provide a [ComfyUI Usage Guide](./ComfyUI/README.md) for HunyuanVideo-1.5.\n\n- **Community-implemented ComfyUI Plugin** - [comfyui_hunyuanvideo_1.5_plugin](https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin): A community-implemented ComfyUI plugin for HunyuanVideo-1.5, offering both simplified and complete node sets for quick usage or deep workflow customization, with built-in automatic model download support.\n\n- **LightX2V** - [LightX2V](https://github.com/ModelTC/LightX2V): A lightweight and efficient video generation framework that integrates HunyuanVideo-1.5, supporting multiple engineering acceleration techniques for fast inference.\n\n- **Wan2GP v9.62** - [Wan2GP](https://github.com/deepbeepmeep/Wan2GP): WanGP is a very low VRAM app (as low 6 GB of VRAM for Hunyuan Video 1.5) supports Lora Accelerator for a 8 steps generation and offers tools to facilitate Video Generation.\n\n\n## \ud83d\udcd1 Open-source Plan\n- HunyuanVideo-1.5 (T2V/I2V)\n  - [x] Inference Code and checkpoints\n  - [x] ComfyUI Support\n  - [x] LightX2V Support\n  - [ ] Diffusers Support\n  - [ ] Release all model weights (Sparse attention, distill model, and SR models)\n\n## \ud83d\udccb Table of Contents\n- [\ud83d\udd25\ud83d\udd25\ud83d\udd25 News](#-news)\n- [\ud83c\udfa5 Demo](#-demo)\n- [\ud83e\udde9 Community Contributions](#-community-contributions)\n- [\ud83d\udcd1 Open-source Plan](#-open-source-plan)\n- [\ud83d\udcd6 Introduction](#-introduction)\n- [\u2728 Key Features](#-key-features)\n- [\ud83d\udcdc System Requirements](#-system-requirements)\n- [\ud83d\udee0\ufe0f Dependencies and Installation](#\ufe0f-dependencies-and-installation)\n- [\ud83e\uddf1 Download Pretrained Models](#-download-pretrained-models)\n- [\ud83d\udcdd Prompt Guide](#-prompt-guide)\n- [\ud83d\udd11 Usage](#-usage)\n  - [Prompt Enhancement](#prompt-enhancement)\n  - [Text to Video](#text-to-video)\n  - [Image to Video](#image-to-video)\n  - [Command Line Arguments](#command-line-arguments)\n  - [Optimal Inference Configurations](#optimal-inference-configurations)\n- [\ud83e\uddf1 Models Cards](#-models-cards)\n- [\ud83c\udfac More Examples](#-more-examples)\n- [\ud83d\udcca Evaluation](#-evaluation)\n- [\ud83d\udcda Citation](#-citation)\n- [\ud83d\ude4f Acknowledgements](#-acknowledgements)\n- [\ud83c\udf1f Github Star History](#-github-star-history)\n\n\n## \ud83d\udcd6 Introduction\nWe present HunyuanVideo-1.5, a lightweight yet powerful video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, includi",
      "description": "HunyuanVideo-1.5 is a lightweight, 8.3B parameter text-to-video generation model offering state-of-the-art quality and motion coherence. It runs efficiently on consumer GPUs, making it accessible for generating creative videos from text or image prompts."
    },
    {
      "id": "Comfy-Org/z_image_turbo",
      "author": "Comfy-Org",
      "author_fullname": "Comfy Org",
      "name": "z_image_turbo",
      "url": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Comfy-Org/z_image_turbo.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/63462c815efccdc07f175568/3ZxGxVKftaTTjduryN7g7.png",
      "downloads": 679715,
      "likes": 262,
      "pipeline_tag": null,
      "num_parameters": null,
      "last_modified": "2025-11-27T04:15:24.000Z",
      "readme_content": "---\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nWorkflows: https://comfyanonymous.github.io/ComfyUI_examples/z_image/",
      "description": "z_image_turbo is a diffusion model designed for single-file image generation, likely integrated with ComfyUI workflows for advanced image synthesis tasks."
    },
    {
      "id": "Supertone/supertonic",
      "author": "Supertone",
      "author_fullname": "Supertone",
      "name": "supertonic",
      "url": "https://huggingface.co/Supertone/supertonic",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Supertone/supertonic.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/68ef1b2d04f0f03d81974536/XltzYlfCoWTzfqimzt5j7.png",
      "downloads": 10545,
      "likes": 344,
      "pipeline_tag": "text-to-speech",
      "num_parameters": null,
      "last_modified": "2025-11-22T01:39:33.000Z",
      "readme_content": "---\nlicense: openrail\nlanguage:\n- en\npipeline_tag: text-to-speech\n---\n\n# Supertonic \u2014 Lightning Fast, On-Device TTS\n\n**Supertonic** is a lightning-fast, on-device text-to-speech system designed for **extreme performance** with minimal computational overhead. Powered by ONNX Runtime, it runs entirely on your device\u2014no cloud, no API calls, no privacy concerns.\n\n> \ud83c\udfa7 **Try it now**: Experience Supertonic in your browser with our [**Interactive Demo**](https://huggingface.co/spaces/Supertone/supertonic#interactive-demo), or [**Hugging Face app**](https://huggingface.co/spaces/akhaliq/supertonic) or get started with pre-trained models from [**Hugging Face Hub**](https://huggingface.co/Supertone/supertonic)\n\n> \ud83d\udee0 **GitHub Repository**  \n> To use Supertonic most easily, visit the official GitHub repository:  \n> https://github.com/supertone-inc/supertonic  \n> You\u2019ll find multi-language example codes.\n\n### Table of Contents\n\n- [Why Supertonic?](#why-supertonic)\n- [Language Support](#language-support)\n- [Getting Started](#getting-started)\n- [Performance](#performance)\n- [Citation](#citation)\n- [License](#license)\n\n## Why Supertonic?\n\n- **\u26a1 Blazingly Fast**: Generates speech up to **167\u00d7 faster than real-time** on consumer hardware (M4 Pro)\u2014unmatched by any other TTS system\n- **\ud83e\udeb6 Ultra Lightweight**: Only **66M parameters**, optimized for efficient on-device performance with minimal footprint\n- **\ud83d\udcf1 On-Device Capable**: **Complete privacy** and **zero latency**\u2014all processing happens locally on your device\n- **\ud83c\udfa8 Natural Text Handling**: Seamlessly processes numbers, dates, currency, abbreviations, and complex expressions without pre-processing\n- **\u2699\ufe0f Highly Configurable**: Adjust inference steps, batch processing, and other parameters to match your specific needs\n- **\ud83e\udde9 Flexible Deployment**: Deploy seamlessly across servers, browsers, and edge devices with multiple runtime backends.\n\n\n## Language Support\n\nWe provide ready-to-use TTS inference examples across multiple ecosystems:\n\n| Language/Platform | Path | Description |\n|-------------------|------|-------------|\n| [**Python**] | `py/` | ONNX Runtime inference |\n| [**Node.js**] | `nodejs/` | Server-side JavaScript |\n| [**Browser**] | `web/` | WebGPU/WASM inference |\n| [**Java**] | `java/` | Cross-platform JVM |\n| [**C++**] | `cpp/` | High-performance C++ |\n| [**C#**] | `csharp/` | .NET ecosystem |\n| [**Go**] | `go/` | Go implementation |\n| [**Swift**] | `swift/` | macOS applications |\n| [**iOS**] | `ios/` | Native iOS apps |\n| [**Rust**] | `rust/` | Memory-safe systems |\n\n> For detailed usage instructions, please refer to the README.md in each language directory.\n\n## Getting Started\n\nFirst, clone the repository:\n\n```bash\ngit clone https://github.com/supertone-inc/supertonic.git\ncd supertonic\n```\n\n### Prerequisites\n\nBefore running the examples, download the ONNX models and preset voices, and place them in the `assets` directory:\n\n```bash\ngit clone https://huggingface.co/Supertone/supertonic assets\n```\n\n> **Note:** The Hugging Face repository uses Git LFS. Please ensure Git LFS is installed and initialized before cloning or pulling large model files.\n> - macOS: `brew install git-lfs && git lfs install`\n> - Generic: see `https://git-lfs.com` for installers\n\n\n### Technical Details\n\n- **Runtime**: ONNX Runtime for cross-platform inference (CPU-optimized; GPU mode is not tested)\n- **Browser Support**: onnxruntime-web for client-side inference\n- **Batch Processing**: Supports batch inference for improved throughput\n- **Audio Output**: Outputs 16-bit WAV files\n\n## Performance\n\nWe evaluated Supertonic's performance (with 2 inference steps) using two key metrics across input texts of varying lengths: Short (59 chars), Mid (152 chars), and Long (266 chars).\n\n**Metrics:**\n- **Characters per Second**: Measures throughput by dividing the number of input characters by the time required to generate audio. Higher is better.\n- **Real-time Factor (RTF)**: Measures the time taken to synthesize audio relative to its duration. Lower is better (e.g., RTF of 0.1 means it takes 0.1 seconds to generate one second of audio).\n\n### Characters per Second\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n|--------|-----------------|----------------|-----------------|\n| **Supertonic** (M4 pro - CPU) | 912 | 1048 | 1263 |\n| **Supertonic** (M4 pro - WebGPU) | 996 | 1801 | 2509 |\n| **Supertonic** (RTX4090) | 2615 | 6548 | 12164 |\n| `API` [ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 144 | 209 | 287 |\n| `API` [OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 37 | 55 | 82 |\n| `API` [Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 12 | 18 | 24 |\n| `API` [Supertone Sona speech 1](https://docs.supertoneapi.com/en/api-reference/endpoints/text-to-speech) | 38 | 64 | 92 |\n| `Open` [Kokoro](https://github.com/hexgrad/kokoro/) | 104 | 107 | 117 |\n| `Open` [NeuTTS Air](https://github.com/neuphonic/neutts-air) | 37 | 42 | 47 |\n\n> **Notes:**  \n> `API` = Cloud-based API services (measured from Seoul)  \n> `Open` = Open-source models  \n> Supertonic (M4 pro - CPU) and (M4 pro - WebGPU): Tested with ONNX  \n> Supertonic (RTX4090): Tested with PyTorch model  \n> Kokoro: Tested on M4 Pro CPU with ONNX  \n> NeuTTS Air: Tested on M4 Pro CPU with Q8-GGUF\n\n### Real-time Factor\n\n| System | Short (59 chars) | Mid (152 chars) | Long (266 chars) |\n|--------|-----------------|----------------|-----------------|\n| **Supertonic** (M4 pro - CPU) | 0.015 | 0.013 | 0.012 |\n| **Supertonic** (M4 pro - WebGPU) | 0.014 | 0.007 | 0.006 |\n| **Supertonic** (RTX4090) | 0.005 | 0.002 | 0.001 |\n| `API` [ElevenLabs Flash v2.5](https://elevenlabs.io/docs/api-reference/text-to-speech/convert) | 0.133 | 0.077 | 0.057 |\n| `API` [OpenAI TTS-1](https://platform.openai.com/docs/guides/text-to-speech) | 0.471 | 0.302 | 0.201 |\n| `API` [Gemini 2.5 Flash TTS](https://ai.google.dev/gemini-api/docs/speech-generation) | 1.060 | ",
      "description": "Supertonic is an ultra-lightweight, on-device text-to-speech system optimized for extreme performance and privacy, achieving over 167x faster than real-time synthesis with minimal computational overhead."
    },
    {
      "id": "PrimeIntellect/INTELLECT-3",
      "author": "PrimeIntellect",
      "author_fullname": "Prime Intellect",
      "name": "INTELLECT-3",
      "url": "https://huggingface.co/PrimeIntellect/INTELLECT-3",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/PrimeIntellect/INTELLECT-3.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e020e4a343274bb132e138/H2mcdPRWtl4iKLd-OYYBc.jpeg",
      "downloads": 1561,
      "likes": 143,
      "pipeline_tag": "text-generation",
      "num_parameters": "106.9B",
      "last_modified": "2025-11-27T03:51:19.000Z",
      "readme_content": "---\nlibrary_name: transformers\ntags:\n- prime-rl\n- verifiers\n- prime-intellect\n- reinforcement-learning\n- reasoning\n- agentic\n- mixture-of-experts\nlicense: mit\nlanguage:\n- en\nbase_model:\n- zai-org/GLM-4.5-Air-Base\npipeline_tag: text-generation\n---\n\n# INTELLECT-3\n\n<div align=\"center\">\n<img src=\"banner.png\" alt=\"Prime Intellect Logo\" />\n</div>\n\n<p align=\"center\">\n    <strong>INTELLECT-3: A 100B+ MoE trained with large-scale RL</strong>\n    <br><br>\n    Trained with <a href=\"https://github.com/PrimeIntellect-ai/prime-rl\">prime-rl</a> and <a href=\"https://github.com/PrimeIntellect-ai/verifiers\">verifiers</a>\n    <br>\n    Environments released on <a href=\"https://app.primeintellect.ai/dashboard/environments\">Environments Hub</a> \n    <br>\n    Read the <a href=\"https://primeintellect.ai/blog/intellect-3\">Blog</a> & <a href=\"https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf\">Technical Report</a>\n    <br>\n    <a href=\"https://x.com/primeintellect\">X</a>  | <a href=\"https://discord.gg/RC5GvMbfDf\">Discord</a> | <a href=\"https://app.primeintellect.ai/dashboard/create-cluster\">Prime Intellect Platform</a>\n</p>\n\n## Introduction\n\n**INTELLECT-3** is a 106B (A12B) parameter Mixture-of-Experts reasoning model post-trained from [GLM-4.5-Air-Base](https://huggingface.co/zai-org/GLM-4.5-Air-Base) using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL).\n\n![bench](bench.png)\n\nTraining was performed with [prime-rl](https://github.com/PrimeIntellect-ai/prime-rl) using environments built with the [verifiers](https://github.com/PrimeIntellect-ai/verifiers) library.\nAll training and evaluation environments are available on the [Environments Hub](https://app.primeintellect.ai/dashboard/environments).\n\nThe model, training frameworks, and environments are open-sourced under fully-permissive licenses (MIT and Apache 2.0).\n\nFor more details, see the [technical report](https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf).\n\n## Evaluation\n\nINTELLECT-3 achieves best-in-class performance on math, coding, and reasoning benchmarks:\n\n| Benchmark | MATH-500 | AIME24 | AIME25 | LCB | GPQA | HLE | MMLU-Pro |\n|-----------|----------|---------|---------|--------|------|-----|----------|\n| INTELLECT-3 | **98.1** | **90.8** | **88.0** | 69.3 | 74.4 | 14.6 | 81.9 |\n| GLM-4.5-Air | 97.8 | 84.6 | 82.0 | 61.5 | 73.3 | 13.3 | 73.9 |\n| GLM-4.5 | 97.0 | 85.8 | 83.3 | 64.5 | 77.0 | 14.8 | 83.5 |\n| DeepSeek R1 0528 | 87.3 | 83.2 | 73.4 | 62.5 | 77.5 | 15.9 | 75.3 |\n| DeepSeek v3.2 | 96.8 | 88.1 | 84.7 | **71.6** | **81.4** | **17.9** | **84.6** |\n| GPT-O5S 120B | 96.0 | 75.8 | 77.7 | 69.9 | 70.0 | 10.6 | 67.1 |\n\n## Model Variants\n\n| Model | HuggingFace |\n|-------|-------------|\n| INTELLECT-3 | [PrimeIntellect/INTELLECT-3](https://huggingface.co/PrimeIntellect/INTELLECT-3) |\n| INTELLECT-3-FP8 | [PrimeIntellect/INTELLECT-3-FP8](https://huggingface.co/PrimeIntellect/INTELLECT-3-FP8) |\n\n## Serving with vLLM\n\nThe BF16 version can be served on 2x H200s:\n```bash\nvllm serve PrimeIntellect/INTELLECT-3 \\\n    --tensor-parallel-size 2 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser qwen3_coder \\\n    --reasoning-parser deepseek_r1\n```\n\nThe FP8 version can be served on a single H200:\n\n```bash\nvllm serve PrimeIntellect/INTELLECT-3-FP8 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser qwen3_coder \\\n    --reasoning-parser deepseek_r1\n```\n\n## Citation\n\n```bibtex\n@misc{intellect3,\n  title={INTELLECT-3: Technical Report},\n  author={Prime Intellect Team},\n  year={2025},\n  url={https://huggingface.co/PrimeIntellect/INTELLECT-3}\n}\n```\n",
      "description": "INTELLECT-3 is a 106B parameter Mixture-of-Experts reasoning model, post-trained with large-scale RL, achieving state-of-the-art performance on math, coding, and reasoning benchmarks. It is designed for complex problem-solving and agentic tasks, with variants optimized for efficient serving."
    }
  ],
  "limit": 10,
  "fetched_at": "2025-11-30T08:42:37.719258+00:00",
  "has_descriptions": true
}