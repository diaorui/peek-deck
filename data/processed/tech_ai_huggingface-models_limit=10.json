{
  "models": [
    {
      "id": "Tongyi-MAI/Z-Image-Turbo",
      "author": "Tongyi-MAI",
      "author_fullname": "Tongyi-MAI",
      "name": "Z-Image-Turbo",
      "url": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Tongyi-MAI/Z-Image-Turbo.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "downloads": 169515,
      "likes": 2177,
      "pipeline_tag": "text-to-image",
      "num_parameters": null,
      "last_modified": "2025-12-02T11:54:58.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n\n<h1 align=\"center\">\u26a1\ufe0f- Image<br><sub><sup>An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</sup></sub></h1>\n\n<div align=\"center\">\n\n[![Official Site](https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage)](https://tongyi-mai.github.io/Z-Image-blog/)&#160;\n[![GitHub](https://img.shields.io/badge/GitHub-Z--Image-181717?logo=github&logoColor=white)](https://github.com/Tongyi-MAI/Z-Image)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Online_Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Mobile_Demo-Z--Image--Turbo-red)](https://huggingface.co/spaces/akhaliq/Z-Image-Turbo)&#160;\n[![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)&#160;\n[![ModelScope Space](https://img.shields.io/badge/\ud83e\udd16%20Online_Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%253A%252F%252FTongyi-MAI%252FZ-Image-Turbo%253Frevision%253Dmaster%7D%7BOnline)&#160;\n[![Art Gallery PDF](https://img.shields.io/badge/%F0%9F%96%BC%20Art_Gallery-PDF-ff69b4)](assets/Z-Image-Gallery.pdf)&#160;\n[![Web Art Gallery](https://img.shields.io/badge/%F0%9F%8C%90%20Web_Art_Gallery-online-00bfff)](https://modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery/summary)&#160;\n<a href=\"https://arxiv.org/abs/2511.22699\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv\" height=\"21px\"></a>\n\n\nWelcome to the official repository for the Z-Image\uff08\u9020\u76f8\uff09project!\n\n</div>\n\n\n\n## \u2728 Z-Image\n\nZ-Image is a powerful and highly efficient image generation model with **6B** parameters. Currently there are three variants:\n\n- \ud83d\ude80 **Z-Image-Turbo** \u2013 A distilled version of Z-Image that matches or exceeds leading competitors with only **8 NFEs** (Number of Function Evaluations). It offers **\u26a1\ufe0fsub-second inference latency\u26a1\ufe0f** on enterprise-grade H800 GPUs and fits comfortably within **16G VRAM consumer devices**. It excels in photorealistic image generation, bilingual text rendering (English & Chinese), and robust instruction adherence.\n\n- \ud83e\uddf1 **Z-Image-Base** \u2013 The non-distilled foundation model. By releasing this checkpoint, we aim to unlock the full potential for community-driven fine-tuning and custom development.\n\n- \u270d\ufe0f **Z-Image-Edit** \u2013 A variant fine-tuned on Z-Image specifically for image editing tasks. It supports creative image-to-image generation with impressive instruction-following capabilities, allowing for precise edits based on natural language prompts.\n\n### \ud83d\udce5 Model Zoo\n\n| Model | Hugging Face                                                                                                                                                                                                                                                                                                              | ModelScope                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| :--- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Z-Image-Turbo** | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Checkpoint%20-Z--Image--Turbo-yellow)](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) <br> [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Online%20Demo-Z--Image--Turbo-blue)](https://huggingface.co/spaces/Tongyi-MAI/Z-Image-Turbo) | [![ModelScope Model](https://img.shields.io/badge/\ud83e\udd16%20%20Checkpoint-Z--Image--Turbo-624aff)](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) <br> [![ModelScope Space](https://img.shields.io/badge/%F0%9F%A4%96%20Online%20Demo-Z--Image--Turbo-17c7a7)](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=469191&modelType=Checkpoint&sdVersion=Z_IMAGE_TURBO&modelUrl=modelscope%3A%2F%2FTongyi-MAI%2FZ-Image-Turbo%3Frevision%3Dmaster) |\n| **Z-Image-Base** | *To be released*                                                                                                                                                                                                                                                                                                          | *To be released*                                                                                                                                                                                                                                                                      ",
      "description": "Z-Image-Turbo is an efficient text-to-image diffusion transformer model optimized for speed and resource usage, achieving sub-second inference with 8 NFEs and fitting within 16GB VRAM. It excels at photorealistic generation, bilingual text rendering (English/Chinese), and strong instruction adherence, making it suitable for rapid content creation on consumer hardware."
    },
    {
      "id": "deepseek-ai/DeepSeek-V3.2",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-V3.2",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-V3.2.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 18140,
      "likes": 743,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-12-01T11:04:59.000Z",
      "readme_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* \ud83e\udd47 **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>hello<\uff5cAssistant\uff5c></think>Hello! I am DeepSeek.<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output ",
      "description": "DeepSeek-V3.2 is an efficient text generation model excelling in reasoning and agentic tasks, featuring DeepSeek Sparse Attention for long contexts and advanced RL training that rivals GPT-5, making it suitable for complex problem-solving and tool-use scenarios."
    },
    {
      "id": "deepseek-ai/DeepSeek-V3.2-Speciale",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-V3.2-Speciale",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-V3.2-Speciale.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 4718,
      "likes": 516,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-12-01T11:06:03.000Z",
      "readme_content": "---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf\"><b>Technical Report</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* \ud83e\udd47 **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align=\"center\">\n <img src=\"assets/benchmark.png\" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a \"thinking with tools\" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model's text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\nencode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: \"<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>hello<\uff5cAssistant\uff5c></think>Hello! I am DeepSeek.<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c><think>\"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output ",
      "description": "DeepSeek-V3.2-Speciale is a highly efficient text generation model fine-tuned from DeepSeek-V3.2-Exp-Base, excelling in reasoning and agentic tasks with performance surpassing GPT-5. It features DeepSeek Sparse Attention for long contexts and a scalable RL framework, making it suitable for complex interactive environments and competitive programming benchmarks."
    },
    {
      "id": "microsoft/VibeVoice-Realtime-0.5B",
      "author": "microsoft",
      "author_fullname": "Microsoft",
      "name": "VibeVoice-Realtime-0.5B",
      "url": "https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/microsoft/VibeVoice-Realtime-0.5B.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "downloads": 20067,
      "likes": 339,
      "pipeline_tag": "text-to-speech",
      "num_parameters": "1.0B",
      "last_modified": "2025-12-05T01:45:28.000Z",
      "readme_content": "---\nlicense: mit\nlanguage:\n- en\npipeline_tag: text-to-speech\ntags:\n- Realtime TTS\n- Streaming text input\n- Long-from speech generation\nlibrary_name: transformers\n---\n\n## VibeVoice: A Frontier Open-Source Text-to-Speech Model\n\nVibeVoice-Realtime is a **lightweight real\u2011time** text-to-speech model supporting **streaming text input** and **robust long-form speech generation**. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in **~300 ms** (hardware dependent).\n\n[\u25b6\ufe0f Watch demo video](https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc) (Launch your own realtime demo via the websocket example in [Usage](https://github.com/microsoft/VibeVoice/blob/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo))\n\nThe model uses an interleaved, windowed design: it incrementally encodes incoming text chunks while, in parallel, continuing diffusion-based acoustic latent generation from prior context. Unlike the full multi-speaker long-form variants, this streaming model removes the semantic tokenizer and relies solely on an efficient acoustic tokenizer operating at an ultra-low frame rate (7.5 Hz).\n\nKey features:\n- Parameter size: 0.5B (deployment-friendly)\n- Realtime TTS (~300 ms first audible latency)\n- Streaming text input\n- Robust long-form speech generation\n\n<p align=\"left\">\n  <img src=\"figures/Fig1.png\" alt=\"VibeVoice Realtime Model Overview\" height=\"250px\">\n</p>\n\nThis realtime variant supports only a single speaker. For multi-speaker conversational speech generation, please use other [VibeVoice models](https://huggingface.co/collections/microsoft/vibevoice). The model is currently intended for English speech only; other languages may produce unpredictable results.\n\n\u27a1\ufe0f **Technical Report:** [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)\n\n\u27a1\ufe0f **Project Page:** [microsoft/VibeVoice](https://microsoft.github.io/VibeVoice)\n\n\u27a1\ufe0f **Code:** [microsoft/VibeVoice-Code](https://github.com/microsoft/VibeVoice)\n\n\n## Training Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic tokenizer and a diffusion-based decoding head.\n- LLM: [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) for this release.\n- Tokenizers:\n    - Acoustic Tokenizer: Based on a \u03c3-VAE variant (proposed in [LatentLM](https://arxiv.org/pdf/2412.08635)), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Decoder component is ~340M parameters.\n- Diffusion Head: Lightweight module (4 layers, ~40M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\n- Context Length: Trained with a curriculum increasing up to 8,192 tokens.\n- Training Stages:\n    - Tokenizer Pre-training: Acoustic tokenizer is pre-trained.\n    - VibeVoice Training: Pre-trained tokenizer is frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 8K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic tokenizer.\n\n\n## Models\n| Model | Context Length | Generation Length |  Weight |\n|-------|----------------|----------|----------|\n| VibeVoice-Realtime-0.5B | 8k | ~10 min | You are here. |\n| VibeVoice-1.5B | 64K | ~90 min | [HF link](https://huggingface.co/microsoft/VibeVoice-1.5B) |\n| VibeVoice-Large| 32K | ~45 min | [HF link](https://huggingface.co/microsoft/VibeVoice-Large) |\n\n\n## Results\n\nThe model achieves satisfactory performance on short-sentence benchmarks, while the model is more focused on long\u2011form speech generation.\n\n### Zero-shot TTS performance on LibriSpeech test-clean set\n\n| Model | WER (%) \u2193 | Speaker Similarity \u2191 |\n|:--------------------|:---------:|:----------------:|\n| VALL-E 2            | 2.40      | 0.643            |\n| Voicebox            | 1.90      | 0.662            |\n| MELLE               | 2.10      | 0.625            |\n| **VibeVoice-Realtime-0.5B** | 2.00 | 0.695            |\n\n### Zero-shot TTS performance on SEED test-en set\n\n| Model | WER (%) \u2193 | Speaker Similarity \u2191 |\n|:--------------------|:---------:|:----------------:|\n| MaskGCT             | 2.62      | 0.714            |\n| Seed-TTS            | 2.25      | 0.762            |\n| FireRedTTS          | 3.82      | 0.460            |\n| SparkTTS            | 1.98      | 0.584            |\n| CosyVoice2          | 2.57      | 0.652            |\n| **VibeVoice-Realtime-0.5B** | 2.05 | 0.633            | \n\n\n## Installation and Usage\n\nPlease refer to [GitHub README](https://github.com/microsoft/VibeVoice/blob/main/docs/vibevoice-realtime-0.5b.md#installation)\n\n\n## Responsible Usage\n### Direct intended uses\nThe VibeVoice-Realtime model is limited to research purposes exploring real-time highly realistic audio generation detailed in the [tech report](https://arxiv.org/pdf/2508.19205). \n\n### Out-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\n\n- Voice impersonation without explicit, recorded consent, including but not limited to, cloning a real individual\u2019s voice for satire, advertising, ransom, social\u2011engineering, or authentication bypass. \n- Disinformation or impersonation, including but not limited to, creating audio presented as genuine recordings of real people or events. \n- Real\u2011time or low\u2011latency voice conversion, including but not limited to, telepho",
      "description": "VibeVoice-Realtime-0.5B is a lightweight, real-time text-to-speech model optimized for streaming input and long-form generation, achieving first audible speech in ~300ms. It's ideal for building real-time TTS services, narrating live data, and enabling LLMs to speak concurrently with text generation."
    },
    {
      "id": "nvidia/Nemotron-Orchestrator-8B",
      "author": "nvidia",
      "author_fullname": "NVIDIA",
      "name": "Nemotron-Orchestrator-8B",
      "url": "https://huggingface.co/nvidia/Nemotron-Orchestrator-8B",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/nvidia/Nemotron-Orchestrator-8B.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1613114437487-60262a8e0703121c822a80b6.png",
      "downloads": 2530,
      "likes": 342,
      "pipeline_tag": "text-generation",
      "num_parameters": "8.2B",
      "last_modified": "2025-12-02T06:56:58.000Z",
      "readme_content": "---\nlibrary_name: transformers\nbase_model:\n- Qwen/Qwen3-8B\n---\n# ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration\n\n[![Paper](https://img.shields.io/badge/ArXiv-Paper-brown)](https://arxiv.org/abs/2511.21689)\n[![Code](https://img.shields.io/badge/GitHub-Link-orange)](https://github.com/NVlabs/ToolOrchestra/)\n[![Model](https://img.shields.io/badge/HuggingFace-Model-green)](https://huggingface.co/nvidia/Orchestrator-8B)\n[![Data](https://img.shields.io/badge/HuggingFace-Data-blue)](https://huggingface.co/datasets/nvidia/ToolScale)\n[![Website](https://img.shields.io/badge/Web-Page-purple)](https://research.nvidia.com/labs/lpr/ToolOrchestra/)\n\n\n### Description\n\nOrchestrator-8B is a state-of-the-art 8B parameter orchestration model designed to solve complex, multi-turn agentic tasks by coordinating a diverse set of expert models and tools.\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/method.png\" width=\"100%\"/>\n<p>\n\n\nOn the Humanity's Last Exam (HLE) benchmark, ToolOrchestrator-8B achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being approximately 2.5x more efficient.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/HLE_benchmark.png\" width=\"80%\"/>\n<p>\n\nThis model is for research and development only.\n\n\n### Key Features\n\n- Intelligent Orchestration: Capable of managing heterogeneous toolsets including basic tools (search, code execution) and other LLMs (specialized and generalist).\n- Multi-Objective RL Training: Trained via Group Relative Policy Optimization (GRPO) with a novel reward function that optimizes for accuracy, latency/cost, and adherence to user preferences.\n- Efficiency: Delivers higher accuracy at significantly lower computational cost compared to monolithic frontier models.\n- Robust Generalization: Demonstrated ability to generalize to unseen tools and pricing configurations.\n\n### Benchmark\nOn Humanity\u2019s Last Exam, Orchestrator-8B achieves 37.1%, surpassing GPT-5 (35.1%) with only 30% monetary cost and 2.5x faster. On FRAMES and \u03c4\u00b2-Bench, Orchestrator-8B consistently outperforms strong monolithic systems, demonstrating versatile reasoning and robust tool orchestration.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/results.png\" width=\"100%\"/>\n<p>\n\nOrchestrator-8B consistently outperforms GPT-5, Claude Opus 4.1 and Qwen3-235B-A22B on HLE with substantially lower cost.\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/NVlabs/ToolOrchestra/main/assets/cost_performance.png\" width=\"60%\"/>\n<p>\n\n\n### Model Details\n\n- Developed by: NVIDIA & University of Hong Kong \n- Model Type: Decoder-only Transformer\n- Base Model: [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) \n- Parameters: 8B\n- Language(s): English\n- License: NVIDIA License\n\n### Model Version(s):\n1.0 <br>\n\n### Training Dataset:\n**Link:** \n| Dataset                      | Link                                                                                         | \n|---------------------------|-------------------------------------------------------------------------------------------|\n| GeneralThought-430K  | [Link](https://huggingface.co/datasets/natolambert/GeneralThought-430K-filtered)                   |\n| ToolScale           | [Link](https://huggingface.co/datasets/nvidia/ToolScale)                            |\n\n\n\n### Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. <br> \n\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns [here](https://app.intigriti.com/programs/nvidia/nvidiavdp/detail).\n\n\n### License/Terms of Use\n[NVIDIA License](LICENSE)\n\n\n### Citation\nIf you find this model useful, please cite our [paper](https://arxiv.org/abs/2511.21689):\n```\n@misc{toolorchestra,\n      title={ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration}, \n      author={Hongjin Su and Shizhe Diao and Ximing Lu and Mingjie Liu and Jiacheng Xu and Xin Dong and Yonggan Fu and Peter Belcak and Hanrong Ye and Hongxu Yin and Yi Dong and Evelina Bakhturina and Tao Yu and Yejin Choi and Jan Kautz and Pavlo Molchanov},\n      year={2025},\n      eprint={2511.21689},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2511.21689}, \n}\n```",
      "description": "Nemotron-Orchestrator-8B is an 8B parameter model that intelligently orchestrates diverse expert models and tools to solve complex agentic tasks, achieving state-of-the-art performance on benchmarks like HLE with superior efficiency compared to monolithic models."
    },
    {
      "id": "alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union",
      "author": "alibaba-pai",
      "author_fullname": "Alibaba-PAI",
      "name": "Z-Image-Turbo-Fun-Controlnet-Union",
      "url": "https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650342890988-623c6253389748c9f72ca287.png",
      "downloads": 0,
      "likes": 250,
      "pipeline_tag": null,
      "num_parameters": null,
      "last_modified": "2025-12-02T06:54:47.000Z",
      "readme_content": "---\r\nlicense: apache-2.0\r\n---\r\n\r\n# Z-Image-Turbo-Fun-Controlnet-Union\r\n\r\n[![Github](https://img.shields.io/badge/\ud83c\udfac%20Code-Github-blue)](https://github.com/aigc-apps/VideoX-Fun)\r\n\r\n## Model Features\r\n- This ControlNet is added on 6 blocks.\r\n- The model was trained from scratch for 10,000 steps on a dataset of 1 million high-quality images covering both general and human-centric content. Training was performed at 1328 resolution using BFloat16 precision, with a batch size of 64, a learning rate of 2e-5, and a text dropout ratio of 0.10.\r\n- It supports multiple control conditions\u2014including Canny, HED, Depth, Pose and MLSD can be used like a standard ControlNet.\r\n- You can adjust control_context_scale for stronger control and better detail preservation. For better stability, we highly recommend using a detailed prompt. The optimal range for control_context_scale is from 0.65 to 0.80.\r\n\r\n## TODO \r\n- [ ] Train on more data and for more steps.\r\n- [ ] Support inpaint mode.\r\n\r\n## Results\r\n\r\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\r\n  <tr>\r\n    <td>Pose</td>\r\n    <td>Output</td>\r\n  </tr>\r\n  <tr>\r\n    <td><img src=\"asset/pose2.jpg\" width=\"100%\" /></td>\r\n    <td><img src=\"results/pose2.png\" width=\"100%\" /></td>\r\n  </tr>\r\n</table>\r\n\r\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\r\n  <tr>\r\n    <td>Pose</td>\r\n    <td>Output</td>\r\n  </tr>\r\n  <tr>\r\n    <td><img src=\"asset/pose.jpg\" width=\"100%\" /></td>\r\n    <td><img src=\"results/pose.png\" width=\"100%\" /></td>\r\n  </tr>\r\n</table>\r\n\r\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\r\n  <tr>\r\n    <td>Canny</td>\r\n    <td>Output</td>\r\n  </tr>\r\n  <tr>\r\n    <td><img src=\"asset/canny.jpg\" width=\"100%\" /></td>\r\n    <td><img src=\"results/canny.png\" width=\"100%\" /></td>\r\n  </tr>\r\n</table>\r\n\r\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\r\n  <tr>\r\n    <td>HED</td>\r\n    <td>Output</td>\r\n  </tr>\r\n  <tr>\r\n    <td><img src=\"asset/hed.jpg\" width=\"100%\" /></td>\r\n    <td><img src=\"results/hed.png\" width=\"100%\" /></td>\r\n  </tr>\r\n</table>\r\n\r\n<table border=\"0\" style=\"width: 100%; text-align: left; margin-top: 20px;\">\r\n  <tr>\r\n    <td>Depth</td>\r\n    <td>Output</td>\r\n  </tr>\r\n  <tr>\r\n    <td><img src=\"asset/depth.jpg\" width=\"100%\" /></td>\r\n    <td><img src=\"results/depth.png\" width=\"100%\" /></td>\r\n  </tr>\r\n</table>\r\n\r\n## Inference\r\nGo to the VideoX-Fun repository for more details.\r\n\r\nPlease clone the VideoX-Fun repository and create the required directories:\r\n\r\n```sh\r\n# Clone the code\r\ngit clone https://github.com/aigc-apps/VideoX-Fun.git\r\n\r\n# Enter VideoX-Fun's directory\r\ncd VideoX-Fun\r\n\r\n# Create model directories\r\nmkdir -p models/Diffusion_Transformer\r\nmkdir -p models/Personalized_Model\r\n```\r\n\r\nThen download the weights into models/Diffusion_Transformer and models/Personalized_Model.\r\n\r\n```\r\n\ud83d\udce6 models/\r\n\u251c\u2500\u2500 \ud83d\udcc2 Diffusion_Transformer/\r\n\u2502   \u2514\u2500\u2500 \ud83d\udcc2 Z-Image-Turbo/\r\n\u251c\u2500\u2500 \ud83d\udcc2 Personalized_Model/\r\n\u2502   \u2514\u2500\u2500 \ud83d\udce6 Z-Image-Turbo-Fun-Controlnet-Union.safetensors\r\n```\r\n\r\nThen run the file `examples/z_image_fun/predict_t2i_control.py`.",
      "description": "Z-Image-Turbo-Fun-Controlnet-Union is a ControlNet model trained on 1 million images, supporting Canny, HED, Depth, and Pose conditions for detailed image generation and control. It's ideal for applications requiring precise structural or stylistic adherence in image synthesis."
    },
    {
      "id": "apple/starflow",
      "author": "apple",
      "author_fullname": "Apple",
      "name": "starflow",
      "url": "https://huggingface.co/apple/starflow",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/apple/starflow.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg",
      "downloads": 0,
      "likes": 226,
      "pipeline_tag": null,
      "num_parameters": null,
      "last_modified": "2025-12-02T01:12:08.000Z",
      "readme_content": "---\nlicense: apple-amlr\nlanguage:\n- en\ntags:\n- normalizing-flows\n- generative-models\n- art\n- autoregressive-models\n---\n# STARFlow: Scalable Transformer Auto-Regressive Flow\n\n<div align=\"center\">\n  <img src=\"starflow_logo.png\" alt=\"STARFlow Logo\" width=\"300\">\n</div>\n\n<div align=\"center\">\n\n[![arXiv](https://img.shields.io/badge/arXiv-2506.06276-b31b1b.svg)](https://arxiv.org/abs/2506.06276)\n[![arXiv](https://img.shields.io/badge/arXiv-2511.20462-b31b1b.svg)](https://arxiv.org/abs/2511.20462)\n[![NeurIPS](https://img.shields.io/badge/NeurIPS-2025%20Spotlight-blue.svg)](https://neurips.cc/Conferences/2025)\n\n</div>\n\nThis is the official open source release of **STARFlow** and **STARFlow-V**, state-of-the-art transformer autoregressive flow models for high-quality image and video generation.\n\n## \ud83d\udcd6 Overview\n\n**STARFlow** introduces a novel transformer autoregressive flow architecture that combines the expressiveness of autoregressive models with the efficiency of normalizing flows. The model achieves state-of-the-art results in both text-to-image and text-to-video generation tasks.\n\n- **[STARFlow](https://arxiv.org/abs/2506.06276)**:  Scaling Latent Normalizing Flows for High-resolution Image Synthesis (NeurIPS 2025 Spotlight)\n- **[STARFlow-V](https://arxiv.org/abs/2511.20462)**: End-to-End Video Generative Modeling with Normalizing Flows (Arxiv)\n\n\ud83c\udfac **[View Video Results Gallery](https://starflow-v.github.io)** - See examples of generated videos and comparisons\n\n## \ud83d\ude80 Quick Start\n\n### Environment Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/apple/ml-starflow\ncd ml-starflow\n\n# Set up conda environment (recommended)\nbash scripts/setup_conda.sh\n\n# Or install dependencies manually\npip install -r requirements.txt\n```\n\n### Model Checkpoints\n\n**Important**: You'll need to download the pretrained model checkpoints and place them in the `ckpts/` directory. For example:\n\n- `ckpts/starflow_3B_t2i_256x256.pth` - For text-to-image generation\n- `ckpts/starflow-v_7B_t2v_caus_480p_v3.pth` - For text-to-video generation\n\n\n### Text-to-Image Generation\n\nGenerate high-quality images from text prompts:\n\n```bash\n# Basic image generation (256x256)\nbash scripts/test_sample_image.sh \"a film still of a cat playing piano\"\n\n# Custom prompt and settings\ntorchrun --standalone --nproc_per_node 1 sample.py \\\n    --model_config_path \"configs/starflow_3B_t2i_256x256.yaml\" \\\n    --checkpoint_path \"ckpts/starflow_3B_t2i_256x256.pth\" \\\n    --caption \"your custom prompt here\" \\\n    --sample_batch_size 8 \\\n    --cfg 3.6 \\\n    --aspect_ratio \"1:1\" \\\n    --seed 999\n```\n\n### Text-to-Video Generation\n\nGenerate videos from text descriptions:\n\n```bash\n# Basic video generation (480p, ~5 seconds)\nbash scripts/test_sample_video.sh \"a corgi dog looks at the camera\"\n\n# With custom input image for TI2V video generation\nbash scripts/test_sample_video.sh \"a cat playing piano\" \"/path/to/input/image.jpg\"\n\n# Longer video generation (specify target length in frames)\nbash scripts/test_sample_video.sh \"a corgi dog looks at the camera\" \"none\" 241  # ~15 seconds at 16fps\nbash scripts/test_sample_video.sh \"a corgi dog looks at the camera\" \"none\" 481  # ~30 seconds at 16fps\n\n# Advanced video generation\ntorchrun --standalone --nproc_per_node 8 sample.py \\\n    --model_config_path \"configs/starflow-v_7B_t2v_caus_480p.yaml\" \\\n    --checkpoint_path \"ckpts/starflow-v_7B_t2v_caus_480p_v3.pth\" \\\n    --caption \"your video prompt here\" \\\n    --sample_batch_size 1 \\\n    --cfg 3.5 \\\n    --aspect_ratio \"16:9\" \\\n    --out_fps 16 \\\n    --jacobi 1 --jacobi_th 0.001 \\\n    --target_length 161  # Customize video length\n```\n\n## \ud83d\udee0\ufe0f Training\n\n### Image Training\n\nTrain your own STARFlow model for text-to-image generation:\n\n```bash\n# Quick training test\nbash scripts/test_train_image.sh 10 16\n\n# Full training with custom parameters\ntorchrun --standalone --nproc_per_node 8 train.py \\\n    --model_config_path \"configs/starflow_3B_t2i_256x256.yaml\" \\\n    --epochs 100 \\\n    --batch_size 1024 \\\n    --wandb_name \"my_starflow_training\"\n```\n\n### Video Training\n\nTrain STARFlow-V for text-to-video generation:\n\n```bash\n# Quick training test\nbash scripts/test_train_video.sh 10 8\n\n# Resume training from checkpoint\ntorchrun --standalone --nproc_per_node 8 train.py \\\n    --model_config_path \"configs/starflow-v_7B_t2v_caus_480p.yaml\" \\\n    --resume_path \"ckpts/starflow-v_7B_t2v_caus_480p_v3.pth\" \\\n    --epochs 100 \\\n    --batch_size 192\n```\n\n## \ud83d\udd27 Utilities\n\n### Video Processing\n\nExtract individual frames from multi-video grids:\n\n```bash\n# Extract frames from a video containing multiple video grids\npython scripts/extract_image_from_video.py --input_video path/to/video.mp4 --output_dir output/\n\n# Extract images with custom settings\npython scripts/extract_images.py input_file.mp4\n```\n\n## \ud83d\udcc1 Model Architecture\n\n### STARFlow (3B Parameters - Text-to-Image)\n- **Resolution**: 256\u00d7256\n- **Architecture**: 6-block deep-shallow architecture\n- **Text Encoder**: T5-XL\n- **VAE**: SD-VAE\n- **Features**: RoPE positional encoding, mixed precision training\n\n### STARFlow-V (7B Parameters - Text-to-Video)\n- **Resolution**: Up to 640\u00d7480 (480p)\n- **Temporal**: 81 frames (16 FPS = ~5 seconds)\n- **Architecture**: 6-block deep-shallow architecture (full sequence)\n- **Text Encoder**: T5-XL\n- **VAE**: WAN2.2-VAE\n- **Features**: Causal attention, autoregressive generation, variable length support\n\n## \ud83d\udd27 Key Features\n\n- **Autoregressive Flow Architecture**: Novel combination of autoregressive models and normalizing flows\n- **High-Quality Generation**: Competetive FID scores and visual quality to State-of-the-art Diffusion Models\n- **Flexible Resolution**: Support for various aspect ratios and resolutions\n- **Efficient Training**: FSDP support for large-scale distributed training\n- **Fast Sampling**: Block-wise Jacobi iteration for accelerated inference\n- **Text Conditioning**: Advanced text-to-image/video capabilities\n- **Video Generation**: Temporal consistency and smooth motion\n\n## \ud83d\udcca Configuration\n\n### Ke",
      "description": "STARFlow is a state-of-the-art transformer autoregressive flow model for high-quality text-to-image and text-to-video generation, combining autoregressive expressiveness with normalizing flow efficiency for competitive FID scores and accelerated inference."
    },
    {
      "id": "black-forest-labs/FLUX.2-dev",
      "author": "black-forest-labs",
      "author_fullname": "Black Forest Labs",
      "name": "FLUX.2-dev",
      "url": "https://huggingface.co/black-forest-labs/FLUX.2-dev",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/black-forest-labs/FLUX.2-dev.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f7a8f4be90e06da248e0f/m5YoF33abJ09vcwFxt1Mj.png",
      "downloads": 196425,
      "likes": 908,
      "pipeline_tag": "image-to-image",
      "num_parameters": null,
      "last_modified": "2025-11-27T11:17:09.000Z",
      "readme_content": "---\nlanguage:\n- en\nlicense: other\nlicense_name: flux-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt\nextra_gated_prompt: >-\n  By clicking \"Agree\", you agree to the [FLUX [dev] Non-Commercial License\n  Agreement](https://huggingface.co/black-forest-labs/FLUX.2-dev/blob/main/LICENSE.txt)\n  and acknowledge the [Acceptable Use\n  Policy](https://bfl.ai/legal/usage-policy).\ntags:\n- image-generation\n- image-editing\n- flux\n- diffusion-single-file\npipeline_tag: image-to-image\nlibrary_name: diffusers\n---\n\n![Teaser](./teaser_generation.png)\n![Teaser](./teaser_editing.png)\n\n`FLUX.2 [dev]` is a 32 billion parameter rectified flow transformer capable of generating, editing and combining images based on text instructions.\nFor more information, please read our [blog post](https://bfl.ai/blog/flux-2).\n\n# Key Features\n1. State of the art in open text-to-image generation, single-reference editing and multi-reference editing.\n2. No need for finetuning: character, object and style reference without additional training in one model.\n4. Trained using guidance distillation, making `FLUX.2 [dev]` more efficient.\n5. Open weights to drive new scientific research, and empower artists to develop innovative workflows.\n6. Generated outputs can be used for personal, scientific, and commercial purposes, as described in the [FLUX \\[dev\\] Non-Commercial License](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev).\n\n# Usage\nWe provide a reference implementation of `FLUX.2 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux2).\nDevelopers and creatives looking to build on top of `FLUX.2 [dev]` are encouraged to use this as a starting point.\n\n`FLUX.2 [dev]` is also available in both [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [Diffusers](https://github.com/huggingface/diffusers).\n\n### Using with diffusers \ud83e\udde8\n\nFor local deployment on a consumer type graphics card, like an RTX 4090 or an RTX 5090, please see the [diffusers docs](https://github.com/black-forest-labs/flux2/blob/main/docs/flux2_dev_hf.md) on our GitHub page.\n\nAs an example, here's a way to load a 4-bit quantized model with a remote text-encoder on an RTX 4090:\n\n```python\nimport torch\nfrom diffusers import Flux2Pipeline\nfrom diffusers.utils import load_image\nfrom huggingface_hub import get_token\nimport requests\nimport io\n\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\" #quantized text-encoder and DiT. VAE still in bf16\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\n\ndef remote_text_encoder(prompts):\n    response = requests.post(\n        \"https://remote-text-encoder-flux-2.huggingface.co/predict\",\n        json={\"prompt\": prompts},\n        headers={\n            \"Authorization\": f\"Bearer {get_token()}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    prompt_embeds = torch.load(io.BytesIO(response.content))\n\n    return prompt_embeds.to(device)\n\npipe = Flux2Pipeline.from_pretrained(\n    repo_id, text_encoder=None, torch_dtype=torch_dtype\n).to(device)\n\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that start with #FF5733 at the top and transitions to #33FF57 at the bottom.\"\n\n#cat_image = load_image(\"https://huggingface.co/spaces/zerogpu-aoti/FLUX.1-Kontext-Dev-fp8-dynamic/resolve/main/cat.png\")\nimage = pipe(\n    prompt_embeds=remote_text_encoder(prompt),\n    #image=[cat_image] #optional multi-image input\n    generator=torch.Generator(device=device).manual_seed(42),\n    num_inference_steps=50, #28 steps can be a good trade-off\n    guidance_scale=4,\n).images[0]\n\nimage.save(\"flux2_output.png\")\n```\n\n\n---\n\n# Risks\n\nBlack Forest Labs is committed to the responsible development and deployment of our models. Prior to releasing the FLUX.2 family of models, we evaluated and mitigated a number of risks in our model checkpoints and hosted services, including the generation of unlawful content such as child sexual abuse material (CSAM) and nonconsensual intimate imagery (NCII). We implemented a series of pre-release mitigations to help prevent misuse by third parties, with additional post-release mitigations to help address residual risks:\n1. Pre-training mitigation. We filtered pre-training data for multiple categories of \u201cnot safe for work\u201d (NSFW) and known child sexual abuse material (CSAM) to help prevent a user generating unlawful content in response to text prompts or uploaded images. We have partnered with the Internet Watch Foundation, an independent nonprofit organization dedicated to preventing online abuse, to filter known CSAM from the training data.\n2. Post-training mitigation. Subsequently, we undertook multiple rounds of targeted fine-tuning to provide additional mitigation against potential abuse, including both text-to-image (T2I) and image-to-image (I2I) attacks. By inhibiting certain behaviors and suppressing certain concepts in the trained model, these techniques can help to prevent a user generating synthetic CSAM or NCII from a text prompt, or transforming an uploaded image into synthetic CSAM or NCII.\n3. Ongoing evaluation. Throughout this process, we conducted multiple internal and external third-party evaluations of model checkpoints to identify further opportunities for mitigation. External third-party evaluations focused on eliciting CSAM and NCII through adversarial testing with (i) text-only prompts, (ii) a single uploaded reference image with text prompts, and (iii) multiple uploaded reference images with text prompts. Based on this feedback, we conducted further safety fine-tuning to produce our open-weight model (FLUX.2 [dev]).\n4. Release decision. After safety fine-tuning and",
      "description": "FLUX.2-dev is a 32B parameter rectified flow transformer for advanced image generation and editing, excelling at text-to-image, single/multi-reference editing without finetuning, and style/character transfer."
    },
    {
      "id": "deepseek-ai/DeepSeek-Math-V2",
      "author": "deepseek-ai",
      "author_fullname": "DeepSeek",
      "name": "DeepSeek-Math-V2",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-Math-V2",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/deepseek-ai/DeepSeek-Math-V2.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png",
      "downloads": 8960,
      "likes": 636,
      "pipeline_tag": "text-generation",
      "num_parameters": "685.4B",
      "last_modified": "2025-11-27T10:35:52.000Z",
      "readme_content": "---\nlicense: apache-2.0\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-Math-V2\n---\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\"><img alt=\"Homepage\"\n    src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\"/></a>\n  <a href=\"https://chat.deepseek.com/\"><img alt=\"Chat\"\n    src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\"/></a>\n  <a href=\"https://huggingface.co/deepseek-ai\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://discord.gg/Tc7c45Zzu5\"><img alt=\"Discord\"\n    src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\"><img alt=\"Wechat\"\n    src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/deepseek_ai\"><img alt=\"Twitter Follow\"\n    src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\n\n**IMO-ProofBench**\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/IMO-ProofBench.png\">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align=\"center\">\n  <img width=41%\" src=\"https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/Competitions.png\">\n</p>\n\n## 4. Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base.\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThis repository and the model weights are licensed under [the Apache License, Version 2.0 (Apache 2.0)](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).\n",
      "description": "DeepSeek-Math-V2 is a large language model specialized in mathematical reasoning and theorem proving, achieving state-of-the-art results on competitions like IMO and Putnam by employing a self-verification mechanism to ensure proof rigor."
    },
    {
      "id": "mistralai/Mistral-Large-3-675B-Instruct-2512",
      "author": "mistralai",
      "author_fullname": "Mistral AI_",
      "name": "Mistral-Large-3-675B-Instruct-2512",
      "url": "https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/mistralai/Mistral-Large-3-675B-Instruct-2512.png",
      "avatar": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png",
      "downloads": 274,
      "likes": 161,
      "pipeline_tag": null,
      "num_parameters": null,
      "last_modified": "2025-12-03T12:46:57.000Z",
      "readme_content": "---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- es\n- de\n- it\n- pt\n- nl\n- zh\n- ja\n- ko\n- ar\nlicense: apache-2.0\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\nbase_model:\n- mistralai/Mistral-Large-3-675B-Base-2512\ntags:\n- mistral-common\n- compressed-tensors\n---\n\n# Mistral Large 3 675B Instruct 2512\nFrom our family of large models, **Mistral Large 3** is a state-of-the-art general-purpose **Multimodal granular Mixture-of-Experts** model with **41B active parameters** and **675B total parameters** trained from the ground up with 3000 H200s.\n\nThis model is the instruct post-trained version in **FP8**, fine-tuned for instruction tasks, making it ideal for chat, agentic and instruction based use cases.  \nDesigned for reliability and long-context comprehension - It is engineered for production-grade assistants, retrieval-augmented systems, scientific workloads, and complex enterprise workflows.\n\nLearn more in our blog post [here](https://mistral.ai/news/mistral-3).\n\nMistral Large 3 is deployable on-premises in:\n- **FP8** on a single node of B200s or H200s.\n- [NVFP4](https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4) on a single node of H100s or A100s.\n\nWe provide a [BF16](https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16) version if needed.\n\n## Key Features\nMistral Large 3 consists of two main architectural components:\n- **A Granular MoE Language Model with 673B params and 39B active**\n- **A 2.5B Vision Encoder**\n\nThe Mistral Large 3 Instruct model offers the following capabilities:\n- **Vision**: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n- **Multilingual**: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n- **System Prompt**: Maintains strong adherence and support for system prompts.\n- **Agentic**: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Frontier**: Delivers best-in-class performance.\n- **Apache 2.0 License**: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\n- **Large Context Window**: Supports a 256k context window.\n\n## Use Cases\nWith powerful long-context performance, stable and consistent cross-domain behavior, Mistral Large 3 is perfect for:\n- Long Document Understanding\n- Powerful Daily-Driver AI Assistants\n- State-of-the-Art Agentic and Tool-Use Capabilities\n- Enterprise Knowledge Work\n- General Coding Assistant\n\nAnd enterprise-grade use cases requiring frontier capabilities.\n\n## Recommended Settings\n\nWe recommend deploying Large 3 in a client-server configuration with the following best practices:\n\n- **System Prompt**: Define a clear environment and use case, including guidance on how to effectively leverage tools in agentic systems.\n- **Sampling Parameters**: Use a temperature below 0.1 for daily-driver and production environments ; Higher temperatures may be explored for creative use cases - developers are encouraged to experiment with alternative settings.\n- **Tools**: Keep the set of tools well-defined and limit their number to the minimum required for the use case - Avoiding overloading the model with an excessive number of tools.\n- **Vision**: When deploying with vision capabilities, we recommend maintaining an aspect ratio close to 1:1 (width-to-height) for images. Avoiding the use of overly thin or wide images - crop them as needed to ensure optimal performance.\n\n### Known Issues / Limitations\n\n- **Not a dedicated reasoning model**: Dedicated reasoning models can outperform Mistral Large 3 in strict reasoning use cases.\n- **Behind vision-first models in multimodal tasks**: Mistral Large 3 can lag behind models optimized for vision tasks and use cases.\n- **Complex deployment**: Due to its large size and architecture, the model can be challenging to deploy efficiently with constrained resources or at scale.\n\n## Benchmark Results\n\nWe compare Mistral Large 3 to similar sized models.\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/IrPlvUUD-5-Phwi9QSevh.png)\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/fDFEymz4HZNsqFARB4u9Y.png)\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/eMdaAPcjOo8VyoGyFKxrE.png)\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](#vllm)\n\n> [!Note]\n> We sadly didn't have enough time to add Mistral Large 3 to transformers, but we would be very happy for a community contribution by opening a PR to [huggingface/transformers](https://github.com/huggingface/transformers).\n\n### vLLM\n\nWe recommend using this model with [vLLM](https://github.com/vllm-project/vllm).\n\n#### Installation\n\nMake sure to install **vllm >= 1.12.0**:\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.8.6`](https://github.com/mistralai/mistral-common/releases/tag/v1.8.6).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest).\n\n#### Serve\n\nThe Mistral Large 3 Instruct FP8 format can be used on one 8xH200 node. We recommend to use this format if you plan to fine-tuning as it can be more precise than NVFP4 in some situations.\n\n**Simple**\n\nA simple launch command is:\n\n```bash\nvllm serve mistralai/Mistral-Large-3-675B-Instruct-2512 \\\n  --tensor-parallel-size 8 \\\n  --tokenizer_mode mistral --config_format mistral --load_format mistral \\\n  --enable-auto-tool-choice --tool-call-parser mistral\n```\n\nKey p",
      "description": "Mistral Large 3 675B Instruct is a state-of-the-art multimodal MoE model with 41B active parameters, excelling in instruction following, chat, and agentic use cases with native function calling. It supports a 256k context window and multilingual capabilities, making it ideal for enterprise workflows and long document understanding."
    }
  ],
  "limit": 10,
  "fetched_at": "2025-12-06T16:15:36.380638+00:00",
  "has_descriptions": true
}