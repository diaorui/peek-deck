{
  "subreddit": "robotics",
  "posts": [
    {
      "title": "Art installation draws attention for its robot dogs with famous faces",
      "author": "nbcnews",
      "url": "https://www.reddit.com/r/robotics/comments/1pe56c3/art_installation_draws_attention_for_its_robot/",
      "published": 1764867378.0,
      "thumbnail": "https://external-preview.redd.it/cTJ2N2RidXF3NzVnMa2NkyWLusU3EEB1G1quIu5uMib9yhFcaTJpvX4KlXi-.png?width=640&crop=smart&auto=webp&s=5c4b356e7b7139ee64c4e5a27f5c0e40298e7324",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "Marc Raibert on Why Robotics Needs More Transparency",
      "author": "Responsible-Grass452",
      "url": "https://www.reddit.com/r/robotics/comments/1pea59d/marc_raibert_on_why_robotics_needs_more/",
      "published": 1764878463.0,
      "thumbnail": "https://external-preview.redd.it/czZ0M2RhcXZrODVnMXDFs25VAGMQ7jD7P7IJtotqHhQXTfEBoVTtdkOU5sNG.png?width=640&crop=smart&auto=webp&s=1ce77a3ba16429df69e330a6878c87a7c4a26597",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Marc Raibert talks about how robotics demos usually show only the polished successes, even though most of the real progress comes from the failures. The awkward grasps, strange edge cases, and completely unexpected behaviors are where engineers learn the most. He points out that hiding all of that creates a distorted picture of what robotics development actually looks like. What makes his take interesting is that it comes from someone who helped define the modern era of legged robots. Raibert has been around long enough to see how public perception shifts when the shiny videos overshadow the grind behind them. His push for more openness feels less like criticism and more like a reminder of what drew so many people into robotics in the first place: the problem solving, the iteration, and the weird in-between moments where breakthroughs usually begin."
    },
    {
      "title": "A comparison of Figure 03, EngineAI T800, and Tesla Optimus running",
      "author": "heart-aroni",
      "url": "https://www.reddit.com/r/robotics/comments/1pdy9mn/a_comparison_of_figure_03_engineai_t800_and_tesla/",
      "published": 1764849765.0,
      "thumbnail": "https://external-preview.redd.it/MzJ0MW50cGZnNjVnMeHfc8HEPZjy1jF6CEoikLt2YRP45U6zCVh8Ohy2O7jN.png?width=640&crop=smart&auto=webp&s=9d5b374f63f785bf03a43178fe6110a9e373d9ec",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "I created a coding agent specialised for ROS",
      "author": "Ok-Leg3893",
      "url": "https://www.reddit.com/r/robotics/comments/1pe70xu/i_created_a_coding_agent_specialised_for_ros/",
      "published": 1764871420.0,
      "thumbnail": "https://preview.redd.it/gabj81rgwd3g1.png?width=640&crop=smart&auto=webp&s=6b5116c271ebdba47b1d8eb19b9717269ddd7063",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "Optimus: Next-Generation Highly Flexible Hand",
      "author": "AlbatrossHummingbird",
      "url": "https://www.reddit.com/r/robotics/comments/1pde8o5/optimus_nextgeneration_highly_flexible_hand/",
      "published": 1764790874.0,
      "thumbnail": "https://external-preview.redd.it/ZDlxMTY2amJsMTVnMdymnC4DYzDl4nmRFCa29adQLOgOJIM-qnPuk14K7m8J.png?width=640&crop=smart&auto=webp&s=01bded19c440d58e06b168d941a5c4acfb9a52f9",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "Pi0 worked for a entire day",
      "author": "drgoldenpants",
      "url": "https://www.reddit.com/r/robotics/comments/1pdjp2c/pi0_worked_for_a_entire_day/",
      "published": 1764803539.0,
      "thumbnail": "https://external-preview.redd.it/OXE2azkweDdtMjVnMUDnGq3QNGwXnAWzx6NNgwjNH9GfBK1kx4uRQGpYEO_C.png?width=640&crop=smart&auto=webp&s=a287bc81dfb34bbc53da2244a9ebf2ead24bcce0",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "https://www.pi.website/blog/pistar06 New way to add RL to imitation learning After training with Recap, \u03c0*0.6 can make various espresso drinks from 5:30am to 11:30pm"
    },
    {
      "title": "Night test of a laser-based local detection prototype",
      "author": "Independent_Win_Alex",
      "url": "https://www.reddit.com/r/robotics/comments/1pe8nau/night_test_of_a_laserbased_local_detection/",
      "published": 1764874955.0,
      "thumbnail": "https://external-preview.redd.it/YjRhbmk4cGJqODVnMfRJhNI_Lz9eqGNjLw5FUWRuziCQ9-CH03w1J9N3aHMm.png?width=640&crop=smart&auto=webp&s=80da42074c3ee1f06f9eba1a67a80e37bf5b0ca6",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "The video shows: - the system's calm state - the moment an object appears in the zone - the transition to active mode and activation This is very early logic, without optimizations or \"smart\" algorithms\u2014I'm simply testing the principle: is it possible to reliably capture live events this way? Feedback would be appreciated."
    },
    {
      "title": "Robot Arm Item-Picking Demo in a Simulated Supermarket Scene",
      "author": "Individual-Major-309",
      "url": "https://www.reddit.com/r/robotics/comments/1pdwivk/robot_arm_itempicking_demo_in_a_simulated/",
      "published": 1764843661.0,
      "thumbnail": "https://external-preview.redd.it/eXRmNXd5YXl3NTVnMYCv8PAXkGgCffkZ3mU3mCIn1fQ3PLGJd4AHR1j6dS6i.png?width=640&crop=smart&auto=webp&s=d95e093b93b6cf63e90427ce47e7edbaa7e3d828",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "A short demo of an item-picking sequence inside a supermarket-style simulation environment. The robot\u2019s navigation in this clip is teleoperated (not autonomous), and the goal is mainly to show how the pick action and scene interactions behave under the current physics setup. For anyone working with manipulation or sim-based workflows, feedback is welcome on aspects such as: motion quality or controller behavior, grasp sequence setup, physics consistency, scene design considerations for similar tasks. Interested in hearing how others approach supermarket-style manipulation tasks in simulation. BGM: Zatplast"
    },
    {
      "title": "Waymo self-driving car enters active police standoff with passenger inside",
      "author": "BuildwithVignesh",
      "url": "https://www.reddit.com/r/robotics/comments/1pcz5ba/waymo_selfdriving_car_enters_active_police/",
      "published": 1764751715.0,
      "thumbnail": "https://external-preview.redd.it/dGsxcHM0MXdjeTRnMQzCE-oKnmX0A0tzj78iO1FrgUK3uFro-tgp7j8i7UI-.png?width=640&crop=smart&auto=webp&s=6c04322bd908a9bd161244d22c7a57aae89950a5",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Videoshows a driverless Waymo entering a police felony stop perimeter in Downtown LA. The Incident: The vehicle navigated just a few feet from officers during a tense standoff while a suspect was on the ground. The Failure: The planner technically worked (it didn't hit anyone) but it completely failed to read the room. It highlights that current AVs have zero concept of danger or social context beyond basic geometry. Outcome: Passengers were safe, but officers had to shout commands at a car that couldn't understand the urgency. Source : NBC News \ud83d\udd17: https://www.google.com/amp/s/www.nbcnews.com/news/amp/rcna246994"
    },
    {
      "title": "X-VLA: The First Soft-Prompted Robot Foundation Model for Any Robot, Any Task",
      "author": "Soft-Worth-4872",
      "url": "https://www.reddit.com/r/robotics/comments/1pe7g1f/xvla_the_first_softprompted_robot_foundation/",
      "published": 1764872329.0,
      "thumbnail": "https://b.thumbs.redditmedia.com/CXJagVm6hhP8uvT5NnTGTn61vXo4qG5GX2no-3v72_s.jpg",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Hi everyone! At Hugging Face / LeRobot, one of our goals is to make strong, accessible VLA models available to the whole robotics community. Today we\u2019re excited to announce X-VLA in LeRobot, a new soft-prompted robot foundation model that can generalize across embodiments, sensors, and action spaces. We\u2019re releasing 6 checkpoints, including a pretrained base model and a cloth-folding checkpoint that hits 100% success for two straight hours. There is also an uncut 2-hour folding run powered entirely by X-VLA (video + checkpoints). You can check it out here: \ud83d\udc49 https://x.com/jadechoghari/status/1996639961366548597 If you want to try it yourself, you can fine-tune X-VLA on any dataset, with any action dimension, directly through LeRobot: https://huggingface.co/collections/lerobot/xvla Happy tinkering, and would love feedback from the community! \ud83e\uddf5\ud83e\udd16 Docs/Blog: https://huggingface.co/docs/lerobot/en/xvlaPaper from Tsinghua: https://arxiv.org/abs/2510.10274 https://preview.redd.it/fzhq2qd7b85g1.png?width=2282&format=png&auto=webp&s=51d9ae8f9481bd3f0537eda6b6e2ee1d29f1e76a"
    }
  ],
  "fetched_at": "2025-12-04T21:39:32.886037+00:00"
}