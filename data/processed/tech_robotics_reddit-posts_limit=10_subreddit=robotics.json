{
  "subreddit": "robotics",
  "posts": [
    {
      "title": "\"Volonaut\" Airbike: Jet-powered hoverbike landing with advanced stabilization (Prototype Demo)",
      "author": "BuildwithVignesh",
      "url": "https://www.reddit.com/r/robotics/comments/1pequkz/volonaut_airbike_jetpowered_hoverbike_landing/",
      "published": 1764927240.0,
      "thumbnail": "https://external-preview.redd.it/bXpjZXM4Z3R1YzVnMYmws2aqjNo7FWNnYlGMXzjowgQGo8kGrcVFwJs_fSXA.png?width=640&crop=smart&auto=webp&s=9be9686837a689089ebc2baa9128c643f8ed13bc",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "This is the Volonaut Airbike, a prototype by Polish inventor Tomasz Patan. Mechanism: Jet-powered vertical take-off and landing (VTOL). Control: Uses an advanced stabilization system to assist the rider's balance during precision maneuvers. Specs: Carbon fiber frame (30kg), top speed ~100km/h (capped) and flight time ~10 mins. Source: Volonaut \ud83d\udd17 : https://youtu.be/4b0Laxsj_z0?si=8loRPWJWr4v622ii"
    },
    {
      "title": "AGIBOT D1 Pro",
      "author": "Nunki08",
      "url": "https://www.reddit.com/r/robotics/comments/1peuynn/agibot_d1_pro/",
      "published": 1764941052.0,
      "thumbnail": "https://external-preview.redd.it/ZGlmbmhkbHR5ZDVnMcmTqcCmyGxlIJwbMAh9zRk_vOBdKNEWQVCUcGUjD1SH.png?width=640&crop=smart&auto=webp&s=8467a4161ee66696616b006012bb7f536eb613b3",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "AGIBOT on \ud835\udd4f: AGIBOT D1 Pro/Edu Quadruped Robot is not only a reliable helper for scientific research and education but also an eye-catcher for entertainment companionship and commercial demonstrations\uff5e 3.5m/s fast running, 1-2 hours battery life, IP54 dustproof & waterproof, durable and easy to use!: https://x.com/AgiBot_zhiyuan/status/1996928040182464537"
    },
    {
      "title": "Art installation draws attention for its robot dogs with famous faces",
      "author": "nbcnews",
      "url": "https://www.reddit.com/r/robotics/comments/1pe56c3/art_installation_draws_attention_for_its_robot/",
      "published": 1764867378.0,
      "thumbnail": "https://external-preview.redd.it/cTJ2N2RidXF3NzVnMa2NkyWLusU3EEB1G1quIu5uMib9yhFcaTJpvX4KlXi-.png?width=640&crop=smart&auto=webp&s=5c4b356e7b7139ee64c4e5a27f5c0e40298e7324",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "Are we witnessing the end of \u201creal robot data\u201d as the foundation of Embodied AI? Recent results from InternData-A1, GEN-0, and Tesla suggest a shift. (Original post by Felicia)",
      "author": "Individual-Major-309",
      "url": "https://www.reddit.com/r/robotics/comments/1pessi4/are_we_witnessing_the_end_of_real_robot_data_as/",
      "published": 1764934446.0,
      "thumbnail": "https://a.thumbs.redditmedia.com/xYGdExuacIcgGtMT_4UoS63x2i1kRFGFAmyDCxoBwi4.jpg",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "For a long time, many robotics teams believed that real robot interaction data was the only reliable foundation for training generalist manipulation models. But real-world data collection is extremely expensive, slow, and fundamentally limited by human labor. Recent results suggest the landscape is changing. Three industry signals stand out: 1. InternData-A1: Synthetic data beats the strongest real-world dataset Shanghai AI Lab\u2019s new paper InternData-A1 (Nov 2025, arXiv) is the first to show that pure simulation data can match or outperform the best real-robot dataset used to train Pi0. The dataset is massive: 630k+ trajectories 7,434 hours 401M frames 4 robot embodiments, 18 skill types, 70 tasks $0.003 per trajectory generation cost One 8\u00d7RTX4090 workstation \u2192 200+ hours of robot data per day Results: On RoboTwin2.0 (49 bimanual tasks): +5\u20136% success over Pi0 On 9 real-world tasks: +6.2% success Sim-to-Real: 1,600 synthetic samples \u2248 200 real samples (\u22488:1 efficiency) The long-held \u201csimulation quality discount\u201d is shrinking fast. 2. GEN-0 exposes the economic impossibility of scaling real-world teleoperation Cross-validated numbers show: Human teleoperation cost per trajectory: $2\u2013$10 Hardware systems: $30k\u2013$40k 1 billion trajectories \u2192 $2\u201310 billion GEN-0\u2019s own scaling law predicts that laundry alone would require 1B interactions for strong performance. https://preview.redd.it/qd8pkcdpfd5g1.png?width=556&format=png&auto=webp&s=1df2607476d3e63f5ca32edae1bf7319d97f1176 Even with Tesla-level resources, this is not feasible. That\u2019s why GEN-0 relies on distributed UMI collection across thousands of sites instead of traditional teleoperation. 3. Tesla\u2019s Optimus shifts dramatically: from mocap \u2192 human video imitation Timeline: 2022\u20132024: Tesla used full-body mocap suits + VR teleop; operators wore ~30 lb rigs, walked 7 hours/day, paid up to $48/hr. May 21, 2025: Tesla confirms:\u201cOptimus is now learning new tasks directly from human videos.\u201d June 2025: Tesla transitions to a vision-only approach, dropping mocap entirely. Their demo showed Optimus performing tasks like trash disposal, vacuuming, cabinet/microwave use, stirring, tearing paper towels, sorting industrial parts \u2014 all claimed to be controlled by a single end-to-end network. 4. So is real robot data obsolete? Not exactly. These developments indicate a shift, not a disappearance: Synthetic data (InternData-A1) is now strong enough to pre-train generalist policies Distributed real data (GEN-0) remains critical for grounding and calibration Pure video imitation (Tesla) offers unmatched scalability but still needs validation for fine manipulation All major approaches still rely on a small amount of real data for fine-tuning or evaluation Open Questions: Where do you think the field is heading? A synthetic-first paradigm? Video-only learning at scale? Hybrid pipelines mixing sim, video, and small real datasets? Or something entirely new? Curious to hear perspectives from researchers, roboticists, and anyone training embodied agents."
    },
    {
      "title": "Behind-the-scenes footage from the EngineAI T800 shoot \u2014 a direct response to the CG accusations.",
      "author": "Ready_Device8994",
      "url": "https://www.reddit.com/r/robotics/comments/1peipg7/behindthescenes_footage_from_the_engineai_t800/",
      "published": 1764899968.0,
      "thumbnail": "https://external-preview.redd.it/vopCp3AoP3agRM_vVjx1qGQ2dEK42KQd5nQEj5DdiQA.jpeg?width=320&crop=smart&auto=webp&s=b4c4505fe2c220f8942da9db848c634ecdba635b",
      "external_url": "https://www.youtube.com/watch?v=VoytjBgpG28",
      "site_name": "youtube.com",
      "favicon": "https://www.youtube.com/s/desktop/db7db827/img/favicon.ico",
      "description": "Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube."
    },
    {
      "title": "ROS News for the Week of December 2nd, 2025",
      "author": "OpenRobotics",
      "url": "https://www.reddit.com/r/robotics/comments/1pf5ccu/ros_news_for_the_week_of_december_2nd_2025/",
      "published": 1764965493.0,
      "thumbnail": "https://external-preview.redd.it/ZRxQSYvz7RK70LarVkZ0JsNr2rRueei96seBmDJoatU.gif?width=640&crop=smart&s=480960cabf1c914aa731ede8c40bec3c3fb4a693",
      "external_url": "https://discourse.openrobotics.org/t/ros-news-for-the-week-of-december-2nd-2025/51298",
      "site_name": "Open Robotics Discourse",
      "favicon": "https://us1.discourse-cdn.com/flex022/uploads/ros/optimized/3X/4/4/4487d2db196c8ca556e8bfac980ed3850ba5cbd5_2_180x180.png",
      "description": "ROS News for the Week of December 2nd, 2025     ROSCon 2025 videos are now available! If you want a quick summary of the event I put together ROSCon 2025 Recap for the OpenCV Weekly Webinar.       For Giving Tuesday we put together a new campaign for ROS users to become a become a Build Farm Backer. If you\u2019ve every saved a few minutes by running sudo apt install ros-kilted-* instead of compiling from source we would love it if you helped cover our compute costs. Also, for the first time ever, we..."
    },
    {
      "title": "PLA logistics brigade member delivering supplies to frontline positions with the new standard passive exoskeleton during combat training.",
      "author": "Important-Battle-374",
      "url": "https://www.reddit.com/r/robotics/comments/1pen0qf/pla_logistics_brigade_member_delivering_supplies/",
      "published": 1764912844.0,
      "thumbnail": "https://preview.redd.it/dsg58fxs665g1.png?width=640&crop=smart&auto=webp&s=df74ae3986191e470f38017b6aff1d3490a14bfa",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    },
    {
      "title": "Making a Marauder's Map from Harry Potter",
      "author": "davesarmoury",
      "url": "https://www.reddit.com/r/robotics/comments/1pf1agl/making_a_marauders_map_from_harry_potter/",
      "published": 1764956101.0,
      "thumbnail": "https://external-preview.redd.it/lkbyF5afhW6Htup7fUyNQVABCSEpJ946WVXQNrql7uM.jpeg?width=320&crop=smart&auto=webp&s=7bd8c748c2ede63e1e653ef9114a146b3f7c00ea",
      "external_url": "https://www.youtube.com/watch?v=dO32ImnsX-4",
      "site_name": "youtube.com",
      "favicon": "https://www.youtube.com/s/desktop/db7db827/img/favicon.ico",
      "description": "Arthur C. Clarke said \"Any sufficiently advanced technology is indistinguishable from magic\". This is the perfect example of that. We are taking a magical map that previously could only exist in a magical world and bringing it to life using robots, DeepStream, and multiple A6000 GPUs!"
    },
    {
      "title": "A potentially highly efficient image and video tokenizer for LLMs/VLAs.",
      "author": "9cheng",
      "url": "https://www.reddit.com/r/robotics/comments/1peo0w3/a_potentially_highly_efficient_image_and_video/",
      "published": 1764916286.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Since 10 years ago, I have been thinking about the following question in my spare time, mostly as an intellectual challenge just for fun: if you are an engineer tasked to design the visual system of an organism, what would you do? This question is too big, so I worked one small step at a time and see how far I can get. I have summarized my decade journey in the following note: https://arxiv.org/abs/2210.13004 Probably the most interesting part is the last part of the note where I proposed a loss function to learn image patches representation using unsupervised learning. The learned representation is a natural binary vector, rather than typical real vectors or binary vectors from quantization of real vectors. Very preliminary experiments show that it is much more efficient than the representation learned by CNN using supervised learning. Practically, I\u2019m thinking this could be used as an image/video tokenizer for LLMs or related models. However, due to growing family responsibilities, I now have less time to pursue this line of research as a hobby. So I\u2019m posting it here in case anyone finds it interesting or useful."
    },
    {
      "title": "Marc Raibert on Why Robotics Needs More Transparency",
      "author": "Responsible-Grass452",
      "url": "https://www.reddit.com/r/robotics/comments/1pea59d/marc_raibert_on_why_robotics_needs_more/",
      "published": 1764878463.0,
      "thumbnail": "https://external-preview.redd.it/czZ0M2RhcXZrODVnMXDFs25VAGMQ7jD7P7IJtotqHhQXTfEBoVTtdkOU5sNG.png?width=640&crop=smart&auto=webp&s=1ce77a3ba16429df69e330a6878c87a7c4a26597",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Marc Raibert talks about how robotics demos usually show only the polished successes, even though most of the real progress comes from the failures. The awkward grasps, strange edge cases, and completely unexpected behaviors are where engineers learn the most. He points out that hiding all of that creates a distorted picture of what robotics development actually looks like. What makes his take interesting is that it comes from someone who helped define the modern era of legged robots. Raibert has been around long enough to see how public perception shifts when the shiny videos overshadow the grind behind them. His push for more openness feels less like criticism and more like a reminder of what drew so many people into robotics in the first place: the problem solving, the iteration, and the weird in-between moments where breakthroughs usually begin."
    }
  ],
  "fetched_at": "2025-12-05T21:14:56.362406+00:00"
}