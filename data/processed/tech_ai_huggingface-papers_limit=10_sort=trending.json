{
  "papers": [
    {
      "id": "2511.22699",
      "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
      "authors": "Z-Image Team, Huanqia Cai, Sihan Cao et al. (21 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "hf_url": "https://huggingface.co/papers/2511.22699",
      "arxiv_url": "https://arxiv.org/abs/2511.22699",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png",
      "upvotes": 131,
      "num_comments": 2,
      "published_at": "2025-11-27T13:52:07.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image",
      "github_stars": 4389,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2511.22677",
      "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
      "authors": "Dongyang Liu, Peng Gao, David Liu et al. (11 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "hf_url": "https://huggingface.co/papers/2511.22677",
      "arxiv_url": "https://arxiv.org/abs/2511.22677",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22677.png",
      "upvotes": 16,
      "num_comments": 2,
      "published_at": "2025-11-27T13:24:28.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "github_stars": 4408,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2306.08568",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao et al. (10 authors)",
      "organization_name": "microsoft",
      "organization_fullname": "Microsoft",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM",
      "ai_summary": "WizardCoder, a Code LLM fine-tuned with complex instructions using Evol-Instruct, outperforms other open-source and closed LLMs on several code generation benchmarks.",
      "hf_url": "https://huggingface.co/papers/2306.08568",
      "arxiv_url": "https://arxiv.org/abs/2306.08568",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2306.08568.png",
      "upvotes": 30,
      "num_comments": 2,
      "published_at": "2023-06-14T11:18:48.000Z",
      "github_repo": "https://github.com/nlpxucan/WizardLM",
      "github_stars": 9469,
      "project_page": null
    },
    {
      "id": "2410.05779",
      "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
      "authors": "Zirui Guo, Lianghao Xia, Yanhua Yu et al. (5 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
      "ai_summary": "LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times.",
      "hf_url": "https://huggingface.co/papers/2410.05779",
      "arxiv_url": "https://arxiv.org/abs/2410.05779",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05779.png",
      "upvotes": 21,
      "num_comments": 0,
      "published_at": "2024-10-08T04:00:12.000Z",
      "github_repo": "https://github.com/hkuds/lightrag",
      "github_stars": 25354,
      "project_page": null
    },
    {
      "id": "2511.16719",
      "title": "SAM 3: Segment Anything with Concepts",
      "authors": "Nicolas Carion, Laura Gustafson, Yuan-Ting Hu et al. (38 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
      "hf_url": "https://huggingface.co/papers/2511.16719",
      "arxiv_url": "https://arxiv.org/abs/2511.16719",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16719.png",
      "upvotes": 103,
      "num_comments": 4,
      "published_at": "2025-11-20T13:59:56.000Z",
      "github_repo": "https://github.com/facebookresearch/sam3",
      "github_stars": 5172,
      "project_page": "https://ai.meta.com/sam3/"
    },
    {
      "id": "2510.14528",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
      "authors": "Cheng Cui, Ting Sun, Suyin Liang et al. (18 authors)",
      "organization_name": "PaddlePaddle",
      "organization_fullname": "PaddlePaddle",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png",
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency.",
      "hf_url": "https://huggingface.co/papers/2510.14528",
      "arxiv_url": "https://arxiv.org/abs/2510.14528",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
      "upvotes": 102,
      "num_comments": 6,
      "published_at": "2025-10-16T06:18:48.000Z",
      "github_repo": "https://github.com/PaddlePaddle/PaddleOCR",
      "github_stars": 65756,
      "project_page": null
    },
    {
      "id": "2412.20138",
      "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "authors": "Yijia Xiao, Edward Sun, Di Luo et al. (4 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, the multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/TauricResearch/TradingAgents.",
      "ai_summary": "A multi-agent framework using large language models for stock trading simulates real-world trading firms, improving performance metrics like cumulative returns and Sharpe ratio.",
      "hf_url": "https://huggingface.co/papers/2412.20138",
      "arxiv_url": "https://arxiv.org/abs/2412.20138",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20138.png",
      "upvotes": 14,
      "num_comments": 0,
      "published_at": "2024-12-28T07:54:06.000Z",
      "github_repo": "https://github.com/tauricresearch/tradingagents",
      "github_stars": 26054,
      "project_page": null
    },
    {
      "id": "2508.03680",
      "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
      "authors": "Xufang Luo, Yuge Zhang, Zhiyuan He et al. (8 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
      "ai_summary": "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.",
      "hf_url": "https://huggingface.co/papers/2508.03680",
      "arxiv_url": "https://arxiv.org/abs/2508.03680",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03680.png",
      "upvotes": 121,
      "num_comments": 6,
      "published_at": "2025-08-05T13:50:13.000Z",
      "github_repo": "https://github.com/microsoft/agent-lightning",
      "github_stars": 9358,
      "project_page": "https://www.microsoft.com/en-us/research/project/agent-lightning/"
    },
    {
      "id": "2510.22543",
      "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
      "authors": "Yuyang Ding, Chi Zhang, Juntao Li et al. (6 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models.",
      "hf_url": "https://huggingface.co/papers/2510.22543",
      "arxiv_url": "https://arxiv.org/abs/2510.22543",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22543.png",
      "upvotes": 10,
      "num_comments": 1,
      "published_at": "2025-10-26T01:49:38.000Z",
      "github_repo": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "github_stars": 17206,
      "project_page": "https://fapo-rl.github.io/"
    },
    {
      "id": "2511.20462",
      "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
      "authors": "Jiatao Gu, Ying Shen, Tianrong Chen et al. (9 authors)",
      "organization_name": "apple",
      "organization_fullname": "Apple",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg",
      "summary": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
      "ai_summary": "STARFlow-V, a normalizing flow-based video generator, offers end-to-end learning, robust causal prediction, and high-quality video generation with practical sampling efficiency.",
      "hf_url": "https://huggingface.co/papers/2511.20462",
      "arxiv_url": "https://arxiv.org/abs/2511.20462",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20462.png",
      "upvotes": 29,
      "num_comments": 2,
      "published_at": "2025-11-25T11:27:58.000Z",
      "github_repo": "https://github.com/apple/ml-starflow",
      "github_stars": 335,
      "project_page": "https://starflow-v.github.io"
    }
  ],
  "limit": 10,
  "sort": "trending",
  "fetched_at": "2025-12-04T14:32:00.370182+00:00"
}