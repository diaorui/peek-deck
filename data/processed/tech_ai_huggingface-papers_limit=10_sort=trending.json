{
  "papers": [
    {
      "id": "2511.22699",
      "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
      "authors": "Z-Image Team, Huanqia Cai, Sihan Cao et al. (21 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "hf_url": "https://huggingface.co/papers/2511.22699",
      "arxiv_url": "https://arxiv.org/abs/2511.22699",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png",
      "upvotes": 85,
      "num_comments": 2,
      "published_at": "2025-11-27T13:52:07.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image",
      "github_stars": 3543,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2511.22677",
      "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
      "authors": "Dongyang Liu, Peng Gao, David Liu et al. (11 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "hf_url": "https://huggingface.co/papers/2511.22677",
      "arxiv_url": "https://arxiv.org/abs/2511.22677",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22677.png",
      "upvotes": 12,
      "num_comments": 2,
      "published_at": "2025-11-27T13:24:28.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "github_stars": 3573,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2306.08568",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao et al. (10 authors)",
      "organization_name": "microsoft",
      "organization_fullname": "Microsoft",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM",
      "ai_summary": "WizardCoder, a Code LLM fine-tuned with complex instructions using Evol-Instruct, outperforms other open-source and closed LLMs on several code generation benchmarks.",
      "hf_url": "https://huggingface.co/papers/2306.08568",
      "arxiv_url": "https://arxiv.org/abs/2306.08568",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2306.08568.png",
      "upvotes": 30,
      "num_comments": 2,
      "published_at": "2023-06-14T11:18:48.000Z",
      "github_repo": "https://github.com/nlpxucan/WizardLM",
      "github_stars": 9467,
      "project_page": null
    },
    {
      "id": "2511.16719",
      "title": "SAM 3: Segment Anything with Concepts",
      "authors": "Nicolas Carion, Laura Gustafson, Yuan-Ting Hu et al. (38 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
      "hf_url": "https://huggingface.co/papers/2511.16719",
      "arxiv_url": "https://arxiv.org/abs/2511.16719",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16719.png",
      "upvotes": 98,
      "num_comments": 4,
      "published_at": "2025-11-20T13:59:56.000Z",
      "github_repo": "https://github.com/facebookresearch/sam3",
      "github_stars": 4891,
      "project_page": "https://ai.meta.com/sam3/"
    },
    {
      "id": "2410.05779",
      "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
      "authors": "Zirui Guo, Lianghao Xia, Yanhua Yu et al. (5 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
      "ai_summary": "LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times.",
      "hf_url": "https://huggingface.co/papers/2410.05779",
      "arxiv_url": "https://arxiv.org/abs/2410.05779",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05779.png",
      "upvotes": 20,
      "num_comments": 0,
      "published_at": "2024-10-08T04:00:12.000Z",
      "github_repo": "https://github.com/hkuds/lightrag",
      "github_stars": 25004,
      "project_page": null
    },
    {
      "id": "2511.19575",
      "title": "HunyuanOCR Technical Report",
      "authors": "Hunyuan Vision Team, Pengyuan Lyu, Xingyu Wan et al. (26 authors)",
      "organization_name": "Tencent-Hunyuan",
      "organization_fullname": "Tencent Hunyuan",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png",
      "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
      "ai_summary": "HunyuanOCR, a lightweight Vision-Language Model, achieves state-of-the-art performance in OCR tasks through a unified end-to-end architecture combining Vision Transformer and lightweight LLM, supported by data-driven and RL strategies.",
      "hf_url": "https://huggingface.co/papers/2511.19575",
      "arxiv_url": "https://arxiv.org/abs/2511.19575",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19575.png",
      "upvotes": 19,
      "num_comments": 3,
      "published_at": "2025-11-24T12:59:59.000Z",
      "github_repo": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "github_stars": 985,
      "project_page": null
    },
    {
      "id": "2510.14528",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
      "authors": "Cheng Cui, Ting Sun, Suyin Liang et al. (18 authors)",
      "organization_name": "PaddlePaddle",
      "organization_fullname": "PaddlePaddle",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png",
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "hf_url": "https://huggingface.co/papers/2510.14528",
      "arxiv_url": "https://arxiv.org/abs/2510.14528",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
      "upvotes": 98,
      "num_comments": 6,
      "published_at": "2025-10-16T06:18:48.000Z",
      "github_repo": "https://github.com/PaddlePaddle/PaddleOCR",
      "github_stars": 65532,
      "project_page": null
    },
    {
      "id": "2511.18423",
      "title": "General Agentic Memory Via Deep Research",
      "authors": "B. Y. Yan, Chaofan Li, Hongjin Qian et al. (5 authors)",
      "organization_name": "BAAI",
      "organization_fullname": "Beijing Academy of Artificial Intelligence",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png",
      "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
      "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.",
      "hf_url": "https://huggingface.co/papers/2511.18423",
      "arxiv_url": "https://arxiv.org/abs/2511.18423",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png",
      "upvotes": 150,
      "num_comments": 2,
      "published_at": "2025-11-23T07:29:33.000Z",
      "github_repo": "https://github.com/VectorSpaceLab/general-agentic-memory",
      "github_stars": 617,
      "project_page": "https://github.com/VectorSpaceLab/general-agentic-memory"
    },
    {
      "id": "2511.16624",
      "title": "SAM 3D: 3Dfy Anything in Images",
      "authors": "SAM 3D Team, Xingyu Chen, Fu-Jen Chu et al. (23 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
      "ai_summary": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.",
      "hf_url": "https://huggingface.co/papers/2511.16624",
      "arxiv_url": "https://arxiv.org/abs/2511.16624",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16624.png",
      "upvotes": 101,
      "num_comments": 3,
      "published_at": "2025-11-20T13:31:46.000Z",
      "github_repo": "https://github.com/facebookresearch/sam-3d-objects",
      "github_stars": 4281,
      "project_page": "https://ai.meta.com/sam3d/"
    },
    {
      "id": "2510.22543",
      "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
      "authors": "Yuyang Ding, Chi Zhang, Juntao Li et al. (6 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models.",
      "hf_url": "https://huggingface.co/papers/2510.22543",
      "arxiv_url": "https://arxiv.org/abs/2510.22543",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22543.png",
      "upvotes": 10,
      "num_comments": 1,
      "published_at": "2025-10-26T01:49:38.000Z",
      "github_repo": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "github_stars": 17024,
      "project_page": "https://fapo-rl.github.io/"
    }
  ],
  "limit": 10,
  "sort": "trending",
  "fetched_at": "2025-12-02T06:53:25.502066+00:00"
}