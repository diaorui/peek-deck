{
  "papers": [
    {
      "id": "2511.22699",
      "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
      "authors": "Z-Image Team, Huanqia Cai, Sihan Cao et al. (21 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
      "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.",
      "hf_url": "https://huggingface.co/papers/2511.22699",
      "arxiv_url": "https://arxiv.org/abs/2511.22699",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png",
      "upvotes": 143,
      "num_comments": 2,
      "published_at": "2025-11-27T13:52:07.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image",
      "github_stars": 4736,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2511.22677",
      "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
      "authors": "Dongyang Liu, Peng Gao, David Liu et al. (11 authors)",
      "organization_name": "Tongyi-MAI",
      "organization_fullname": "Tongyi-MAI",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg",
      "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
      "ai_summary": "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.",
      "hf_url": "https://huggingface.co/papers/2511.22677",
      "arxiv_url": "https://arxiv.org/abs/2511.22677",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22677.png",
      "upvotes": 18,
      "num_comments": 2,
      "published_at": "2025-11-27T13:24:28.000Z",
      "github_repo": "https://github.com/Tongyi-MAI/Z-Image/tree/main",
      "github_stars": 4751,
      "project_page": "https://tongyi-mai.github.io/Z-Image-blog/"
    },
    {
      "id": "2512.02589",
      "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
      "authors": "Junyi Hou, Andre Lin Huikai, Nuo Chen et al. (5 authors)",
      "organization_name": "NationalUniversityofSingapore",
      "organization_fullname": "National University of Singapore",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png",
      "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
      "ai_summary": "PaperDebugger is an in-editor academic writing assistant that integrates large language models, enabling direct interaction within LaTeX editors for document state management, revision, and literature search.",
      "hf_url": "https://huggingface.co/papers/2512.02589",
      "arxiv_url": "https://arxiv.org/abs/2512.02589",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02589.png",
      "upvotes": 25,
      "num_comments": 2,
      "published_at": "2025-12-02T05:00:37.000Z",
      "github_repo": "https://github.com/PaperDebugger/PaperDebugger",
      "github_stars": 271,
      "project_page": "https://www.paperdebugger.com/"
    },
    {
      "id": "2512.04678",
      "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
      "authors": "Yunhong Lu, Yanhong Zeng, Haobo Li et al. (12 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
      "ai_summary": "The paper introduces Reward Forcing, which enhances video generation by updating sink tokens with EMA-Sink and using Rewarded Distribution Matching Distillation to prioritize dynamic content.",
      "hf_url": "https://huggingface.co/papers/2512.04678",
      "arxiv_url": "https://arxiv.org/abs/2512.04678",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04678.png",
      "upvotes": 31,
      "num_comments": 3,
      "published_at": "2025-12-04T06:12:13.000Z",
      "github_repo": "https://github.com/JaydenLyh/Reward-Forcing",
      "github_stars": 90,
      "project_page": "https://reward-forcing.github.io/"
    },
    {
      "id": "2511.16719",
      "title": "SAM 3: Segment Anything with Concepts",
      "authors": "Nicolas Carion, Laura Gustafson, Yuan-Ting Hu et al. (38 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
      "hf_url": "https://huggingface.co/papers/2511.16719",
      "arxiv_url": "https://arxiv.org/abs/2511.16719",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16719.png",
      "upvotes": 105,
      "num_comments": 4,
      "published_at": "2025-11-20T13:59:56.000Z",
      "github_repo": "https://github.com/facebookresearch/sam3",
      "github_stars": 5298,
      "project_page": "https://ai.meta.com/sam3/"
    },
    {
      "id": "2410.05779",
      "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
      "authors": "Zirui Guo, Lianghao Xia, Yanhua Yu et al. (5 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
      "ai_summary": "LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times.",
      "hf_url": "https://huggingface.co/papers/2410.05779",
      "arxiv_url": "https://arxiv.org/abs/2410.05779",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05779.png",
      "upvotes": 22,
      "num_comments": 0,
      "published_at": "2024-10-08T04:00:12.000Z",
      "github_repo": "https://github.com/hkuds/lightrag",
      "github_stars": 25448,
      "project_page": null
    },
    {
      "id": "2412.20138",
      "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "authors": "Yijia Xiao, Edward Sun, Di Luo et al. (4 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, the multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/TauricResearch/TradingAgents.",
      "ai_summary": "A multi-agent framework using large language models for stock trading simulates real-world trading firms, improving performance metrics like cumulative returns and Sharpe ratio.",
      "hf_url": "https://huggingface.co/papers/2412.20138",
      "arxiv_url": "https://arxiv.org/abs/2412.20138",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20138.png",
      "upvotes": 14,
      "num_comments": 0,
      "published_at": "2024-12-28T07:54:06.000Z",
      "github_repo": "https://github.com/tauricresearch/tradingagents",
      "github_stars": 26203,
      "project_page": null
    },
    {
      "id": "2306.08568",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao et al. (10 authors)",
      "organization_name": "microsoft",
      "organization_fullname": "Microsoft",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM",
      "ai_summary": "WizardCoder, a Code LLM fine-tuned with complex instructions using Evol-Instruct, outperforms other open-source and closed LLMs on several code generation benchmarks.",
      "hf_url": "https://huggingface.co/papers/2306.08568",
      "arxiv_url": "https://arxiv.org/abs/2306.08568",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2306.08568.png",
      "upvotes": 31,
      "num_comments": 2,
      "published_at": "2023-06-14T11:18:48.000Z",
      "github_repo": "https://github.com/nlpxucan/WizardLM",
      "github_stars": 9470,
      "project_page": null
    },
    {
      "id": "2512.04926",
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "authors": "Yueming Pan, Ruoyu Feng, Qi Dai et al. (8 authors)",
      "organization_name": "XianJiaotongUniversity",
      "organization_fullname": "Xi'an Jiaotong University",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png",
      "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
      "ai_summary": "Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.",
      "hf_url": "https://huggingface.co/papers/2512.04926",
      "arxiv_url": "https://arxiv.org/abs/2512.04926",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04926.png",
      "upvotes": 28,
      "num_comments": 2,
      "published_at": "2025-12-04T10:57:27.000Z",
      "github_repo": "https://github.com/yuemingPAN/SFD",
      "github_stars": 75,
      "project_page": "https://yuemingpan.github.io/SFD.github.io/"
    },
    {
      "id": "2510.14528",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
      "authors": "Cheng Cui, Ting Sun, Suyin Liang et al. (18 authors)",
      "organization_name": "PaddlePaddle",
      "organization_fullname": "PaddlePaddle",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png",
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency.",
      "hf_url": "https://huggingface.co/papers/2510.14528",
      "arxiv_url": "https://arxiv.org/abs/2510.14528",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
      "upvotes": 103,
      "num_comments": 6,
      "published_at": "2025-10-16T06:18:48.000Z",
      "github_repo": "https://github.com/PaddlePaddle/PaddleOCR",
      "github_stars": 65849,
      "project_page": null
    }
  ],
  "limit": 10,
  "sort": "trending",
  "fetched_at": "2025-12-06T09:58:41.298045+00:00"
}