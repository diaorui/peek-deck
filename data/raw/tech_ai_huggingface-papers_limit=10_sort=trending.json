{
  "papers": [
    {
      "id": "2306.08568",
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao et al. (10 authors)",
      "organization_name": "microsoft",
      "organization_fullname": "Microsoft",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
      "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM",
      "ai_summary": "WizardCoder, a Code LLM fine-tuned with complex instructions using Evol-Instruct, outperforms other open-source and closed LLMs on several code generation benchmarks.",
      "hf_url": "https://huggingface.co/papers/2306.08568",
      "arxiv_url": "https://arxiv.org/abs/2306.08568",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2306.08568.png",
      "upvotes": 30,
      "num_comments": 1,
      "published_at": "2023-06-14T11:18:48.000Z",
      "github_repo": "https://github.com/nlpxucan/WizardLM",
      "github_stars": 9462,
      "project_page": null
    },
    {
      "id": "2511.16719",
      "title": "SAM 3: Segment Anything with Concepts",
      "authors": "Nicolas Carion, Laura Gustafson, Yuan-Ting Hu et al. (38 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "ai_summary": "Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.",
      "hf_url": "https://huggingface.co/papers/2511.16719",
      "arxiv_url": "https://arxiv.org/abs/2511.16719",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16719.png",
      "upvotes": 91,
      "num_comments": 4,
      "published_at": "2025-11-20T13:59:56.000Z",
      "github_repo": "https://github.com/facebookresearch/sam3",
      "github_stars": 4707,
      "project_page": "https://ai.meta.com/sam3/"
    },
    {
      "id": "2511.16624",
      "title": "SAM 3D: 3Dfy Anything in Images",
      "authors": "SAM 3D Team, Xingyu Chen, Fu-Jen Chu et al. (23 authors)",
      "organization_name": "facebook",
      "organization_fullname": "AI at Meta",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png",
      "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
      "ai_summary": "SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.",
      "hf_url": "https://huggingface.co/papers/2511.16624",
      "arxiv_url": "https://arxiv.org/abs/2511.16624",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16624.png",
      "upvotes": 99,
      "num_comments": 3,
      "published_at": "2025-11-20T13:31:46.000Z",
      "github_repo": "https://github.com/facebookresearch/sam-3d-objects",
      "github_stars": 4152,
      "project_page": "https://ai.meta.com/sam3d/"
    },
    {
      "id": "2511.19575",
      "title": "HunyuanOCR Technical Report",
      "authors": "Hunyuan Vision Team, Pengyuan Lyu, Xingyu Wan et al. (26 authors)",
      "organization_name": "Tencent-Hunyuan",
      "organization_fullname": "Tencent Hunyuan",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png",
      "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
      "ai_summary": "HunyuanOCR, a lightweight Vision-Language Model, achieves state-of-the-art performance in OCR tasks through a unified end-to-end architecture combining Vision Transformer and lightweight LLM, supported by data-driven and RL strategies.",
      "hf_url": "https://huggingface.co/papers/2511.19575",
      "arxiv_url": "https://arxiv.org/abs/2511.19575",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19575.png",
      "upvotes": 16,
      "num_comments": 3,
      "published_at": "2025-11-24T12:59:59.000Z",
      "github_repo": "https://github.com/Tencent-Hunyuan/HunyuanOCR",
      "github_stars": 774,
      "project_page": null
    },
    {
      "id": "2510.14528",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
      "authors": "Cheng Cui, Ting Sun, Suyin Liang et al. (18 authors)",
      "organization_name": "PaddlePaddle",
      "organization_fullname": "PaddlePaddle",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png",
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "hf_url": "https://huggingface.co/papers/2510.14528",
      "arxiv_url": "https://arxiv.org/abs/2510.14528",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
      "upvotes": 98,
      "num_comments": 6,
      "published_at": "2025-10-16T06:18:48.000Z",
      "github_repo": "https://github.com/PaddlePaddle/PaddleOCR",
      "github_stars": 65347,
      "project_page": null
    },
    {
      "id": "2511.19900",
      "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
      "authors": "Jiaqi Liu, Kaiwen Xiong, Peng Xia et al. (9 authors)",
      "organization_name": "UNC-ChapelHill",
      "organization_fullname": "University of North Carolina at Chapel Hill",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png",
      "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.",
      "ai_summary": "Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.",
      "hf_url": "https://huggingface.co/papers/2511.19900",
      "arxiv_url": "https://arxiv.org/abs/2511.19900",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png",
      "upvotes": 45,
      "num_comments": 2,
      "published_at": "2025-11-24T23:15:14.000Z",
      "github_repo": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL",
      "github_stars": 670,
      "project_page": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL"
    },
    {
      "id": "2410.05779",
      "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
      "authors": "Zirui Guo, Lianghao Xia, Yanhua Yu et al. (5 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
      "ai_summary": "LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times.",
      "hf_url": "https://huggingface.co/papers/2410.05779",
      "arxiv_url": "https://arxiv.org/abs/2410.05779",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05779.png",
      "upvotes": 20,
      "num_comments": 0,
      "published_at": "2024-10-08T04:00:12.000Z",
      "github_repo": "https://github.com/hkuds/lightrag",
      "github_stars": 24799,
      "project_page": null
    },
    {
      "id": "2511.18423",
      "title": "General Agentic Memory Via Deep Research",
      "authors": "B. Y. Yan, Chaofan Li, Hongjin Qian et al. (5 authors)",
      "organization_name": "BAAI",
      "organization_fullname": "Beijing Academy of Artificial Intelligence",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png",
      "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
      "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.",
      "hf_url": "https://huggingface.co/papers/2511.18423",
      "arxiv_url": "https://arxiv.org/abs/2511.18423",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png",
      "upvotes": 144,
      "num_comments": 2,
      "published_at": "2025-11-23T07:29:33.000Z",
      "github_repo": "https://github.com/VectorSpaceLab/general-agentic-memory",
      "github_stars": 398,
      "project_page": "https://github.com/VectorSpaceLab/general-agentic-memory"
    },
    {
      "id": "2511.16043",
      "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
      "authors": "Peng Xia, Kaide Zeng, Jiaqi Liu et al. (8 authors)",
      "organization_name": "UNC-ChapelHill",
      "organization_fullname": "University of North Carolina at Chapel Hill",
      "organization_avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png",
      "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.",
      "ai_summary": "Agent0, a self-evolving framework utilizing multi-step co-evolution and tool integration, enhances LLM reasoning capabilities without human-curated data.",
      "hf_url": "https://huggingface.co/papers/2511.16043",
      "arxiv_url": "https://arxiv.org/abs/2511.16043",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16043.png",
      "upvotes": 97,
      "num_comments": 4,
      "published_at": "2025-11-20T00:01:57.000Z",
      "github_repo": "https://github.com/aiming-lab/Agent0",
      "github_stars": 673,
      "project_page": null
    },
    {
      "id": "2510.22543",
      "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
      "authors": "Yuyang Ding, Chi Zhang, Juntao Li et al. (6 authors)",
      "organization_name": null,
      "organization_fullname": null,
      "organization_avatar": null,
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models.",
      "hf_url": "https://huggingface.co/papers/2510.22543",
      "arxiv_url": "https://arxiv.org/abs/2510.22543",
      "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22543.png",
      "upvotes": 10,
      "num_comments": 1,
      "published_at": "2025-10-26T01:49:38.000Z",
      "github_repo": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "github_stars": 16883,
      "project_page": "https://fapo-rl.github.io/"
    }
  ],
  "limit": 10,
  "sort": "trending",
  "fetched_at": "2025-11-30T05:58:46.062258+00:00"
}